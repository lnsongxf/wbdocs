1
The resource “Business Analytics Toolkit for Tech Hubs: Lessons Learned from infoDev’s mLabs and mHubs” is available at
http://www.infodev.org/mobile/tech-hub-business-analytics-toolkit

© 2015 International Bank for Reconstruction and Development/The World Bank, infoDev
1818 H Street NW, Washington DC 20433
Telephone: +1 202-473-1000; Internet: www.worldbank.org


Some rights reserved.

This work is a product of the staff of The World Bank with external contributions. Note that The World Bank does not
necessarily own each component of the content included in the work. The World Bank therefore does not warrant that the use
of the content contained in the work will not infringe on the rights of third parties. The risk of claims resulting from such
infringement rests solely with you.

The findings, interpretations, and conclusions expressed in this work do not necessarily reflect the views of The World Bank, its
Board of Executive Directors, or the governments they represent. The World Bank does not guarantee the accuracy of the data
included in this work. The boundaries, colors, denominations, and other information shown on any map in this work do not
imply any judgment on the part of The World Bank concerning the legal status of any territory or the endorsement or
acceptance of such boundaries.

Nothing herein shall constitute or be considered to be a limitation upon or waiver of the privileges and immunities of The World
Bank, all of which are specifically reserved.

Rights and Permissions




This work is available under the Creative Commons Attribution 3.0 Unported license (CC BY 3.0)
http://creativecommons.org/licenses/by/3.0. Under the Creative Commons Attribution license, you are free to copy, distribute,
transmit, and adapt this work, including for commercial purposes, under the following conditions:

Attribution—Please cite the work as follows: infoDev. 2015. “Business Analytics Toolkit for Tech Hubs: Lessons Learned from
infoDev’s mLabs and mHubs.” Washington, DC: The World Bank. Website: http://www.infodev.org/mobile/tech-hub-business-
analytics-toolkit

License: Creative Commons Attribution CC BY 3.0

Translations—If you create a translation of this work, please add the following disclaimer along with the attribution.
This translation was not created by The World Bank and should not be considered an official World Bank translation. The World
Bank shall not be liable for any content or error in this translation.

All queries on rights and licenses should be addressed to infoDev, The World Bank, 1818 H Street NW, MSN 5K–508,
Washington, DC 20433, USA; telephone: 202-458-8831; Internet: www.infodev.org; email: info@infodev.org.

Cover design: infoDev




                                                                                                                                2
About infoDev
infoDev, a global program of the World Bank Group, supports growth-oriented entrepreneurs through
creative and path-breaking venture enablers. It assists entrepreneurs to secure appropriate early-stage
financing; convening entrepreneurs, investors, policymakers, mentors, and other stakeholders for
dialogue and action. We also produce cutting-edge knowledge products, closely linked to our work on the
ground.


About infoDev’s Digital Entrepreneurship Program
infoDev / World Bank Group’s Digital Entrepreneurship Program (DEP) aims to support competitive digital
entrepreneurs that create new business models, products, and services, to gain access to new
opportunities and contribute to the public and private sectors. We encourage entrepreneurial solutions
for improved government service delivery, banking, health, education, transport and logistics,
transparency, and other development outcomes.


The DEP works with specialized innovation hubs and accelerators, called mLabs and mHubs, to advance
the growth of high-potential startups and small and medium-sized firms. Specifically, it aims to:
      Connect innovative, growth-oriented entrepreneurs with knowledge, capital and markets locally
       and internationally;
      Create a demonstration effect that inspires nascent entrepreneurs and digital innovation
       stakeholders to take action; and
      Allow for scaling up through World Bank Group operations and other development partners.


The backbone of infoDev’s Digital Entrepreneurship Program is a global network of Mobile Application
Laboratories (mLabs) and Mobile Social Networking Hubs (mHubs), rolled out across twelve countries.




                                                                                                     3
Acknowledgements
This toolkit, commissioned by infoDev, a global partnership program within the World Bank, provides
guidance on how to develop business analytics—measuring and learning from the performance and
effects—of mobile application labs (mLabs) and mobile social networking hubs (mHubs). The mLab and
mHub pilots were implemented by infoDev under the Creating Sustainable Businesses in the Knowledge
Economy (CSBKE) program, funded by the government of Finland, in partnership with Nokia.
Nicolas Friederici (Oxford Internet Institute) wrote the content of the toolkit. Toni Eliasz was task manager
for infoDev. The toolkit was informed by interview data and experiences gathered during earlier
evaluations, notably the Business Model Evaluation of mLab and mHub pilots, available at
http://www.infodev.org/mobilebusinessmodels. Catherine Amelink, Maja Andjelkovic, Brett Dickstein,
Zoe Lu, and Sophia Muradyan from infoDev provided essential feedback during an internal review process.


The report was made possible by the support of the Ministry for Foreign Affairs of the Government of
Finland, the Swedish International Development Cooperation Agency (SIDA), and the Norwegian Agency
for Development Cooperation (NORAD).




                                                                                                           4
Contents 
1.      Purpose and Highlights ............................................................................................................................ 7 
2.                              .............................................................................................................................. 8 
        Is this Toolkit for You? 
3.      What Led infoDev to Develop this Toolkit? ............................................................................................. 9 
        Box 1: mLabs and mHubs as Local Nodes of the Digital Entrepreneurship Program’s Global Network
         ................................................................................................................................................................ 10 
4.      Why Are Business Analytics Key to a Tech Hub’s Success? ................................................................... 12 
      4.1.      Using the “Build, Measure, Learn” Principle to Work in Complex Innovation Ecosystems ......... 12 
      4.2.      Six Ways for Tech Hubs to Benefit from Business Analytics ......................................................... 14 
        4.2.1.          Finding Focus for your Vision and Decision‐making ............................................................. 14 
        4.2.2.          Learning and Improving ......................................................................................................... 14 
        4.2.3.          Fundraising ............................................................................................................................ 15 
        4.2.4.          Showcasing Clients ................................................................................................................ 15 
        4.2.5.                                      .................................................................................................. 16 
                        Sharing Success with Clients 
        4.2.6.          Being Accountable ................................................................................................................. 16 
5.  What Does Your Tech Hub Want to Achieve? Your Business Model as the Foundation for Business 
Analytics ......................................................................................................................................................... 17 
6.      Who are Your Funders? The Special Case of Funding from Donors and Governments ....................... 20 
7.      What Should You Measure? Developing a Business Analytics Approach and Selecting Indicators ..... 23 
      7.1.      General Guidelines for Tech Hub Performance Indicator Selection ............................................. 23 
        7.1.1.          Less Can Be More .................................................................................................................. 24 
        7.1.2.          Focus on Your Contribution ................................................................................................... 25 
        7.1.3.          Longitudinal and Before/After Data Is Key ............................................................................ 25 
        7.1.4.                                                           ............................................................... 26 
                        Use Quantitative Information but Target It Wisely 
        7.1.5.          Don’t Underestimate the Qualitative .................................................................................... 26 
        7.1.6.                                                                        .................................... 27 
                        Find the Right Mix Between Consistent and Flexible Measurement 
        7.1.7.          Integrate Performance Tracking in Agreements with Your Clients and Partners ................ 27 
      7.2.      Selecting Indicators and Performance Metrics for Tech Hubs ..................................................... 28 
        7.2.1.                                                     ................................................................... 28 
                        Funder‐Designed vs. Your Own Business Model 
        7.2.2.          Implementation Quality/Output Indicators .......................................................................... 29 
        7.2.3.          Value Proposition/Outcome Indicators ................................................................................. 30 
8.      How Should You Measure? Putting Your Business Analytics Framework into Action .......................... 33 
      8.1.      Establishing a Performance Measurement System ...................................................................... 33 
      8.2.      Use Lean Start‐up‐style Hypothesis Validation to Extract Better Learnings ................................. 34 

                                                                                                                                                                          5 
 
      8.3.       Budgeting, Planning, and Staffing for Continuous Analytics ......................................................... 35 
      8.4.       Involve Outside Help ...................................................................................................................... 36 
9.       How do mLabs and mHubs Work with infoDev? ................................................................................... 37 
      RESOURCES: ............................................................................................................................................... 38 
         Business Model Toolkit for mLabs and mHubs ..................................................................................... 38 
         Resources Including Assessments of Tech hubs ................................................................................... 38 
         Resources Including Market and Mobile Innovation Ecosystem Assessments .................................... 38 
         Other Toolkits......................................................................................................................................... 38 
      APPENDICES: Examples of Indicators and Definitions by mLab/mHub Approach .................................... 38 
         Appendix 1: Implementation Quality Indicators ................................................................................... 38 
                                                         ............................................................................. 41 
         Appendix 2: Value Proposition/Outcome Indicators 
 
                                                    




                                                                                                                                                                 6 
 
1. Purpose and Highlights
This Business Analytics Toolkit will help you:
     1) Understand how to conduct performance measurement for an mLab or mHub, or other tech
        hubs1
     2) Improve your planning, lesson learning, and delivery over time
     3) Collect data needed to communicate to potential investors and partners.
The toolkit:
         Is oriented towards tech hub managers but is also useful for others interested in the design of
          tech hubs (part 2)
         Provides a brief description of what led infoDev to put together this toolkit (part 3)
         Makes the case for the relevance of rigorous business analytics (part 4)
         Categorizes tech hub business models and outlines the consequences of business model selection
          for business analytics strategies (part 5)
         Highlights important considerations for tech hubs that are funded by governments and donors,
          including international development organizations such as infoDev (part 6)
         Gives detailed guidance on how a good business analytics approach can be developed and
          indicators selected in a performance measurement system (part 7)
         Provides instructions on how tech hubs can use business analytics and performance
          measurement in a continuous process (part 8)
         Briefly outlines how mLabs and mHubs can engage with infoDev once they have a sound business
          analytics approach in place (part 9).
This is version 1.0 of this toolkit, and you are encouraged to help improve future versions by submitting
your feedback to infoDev.




1Tech hub in this toolkit refers to all organizations that provide services for early-stage technology entrepreneurs, enabling them
to network, develop technological innovations, and start growth-oriented businesses. In other publications, such organizations
might be called innovation hubs/labs, technology innovation hubs/labs, networked incubators and accelerators, or other similar
terms. Tech hubs across the globe are still evolving and defining their identity and differentiation factors, but overall tech hubs
are a further development of incubation and acceleration approaches that focus broadly on early-stage innovation and systemic
impact in innovation ecosystems, beyond direct business support. Tech hub business models can often be categorized as Start-up
Creation, Skill Development, and Network Building (see section 5) mLabs and mHubs are a specific type of tech hub that function
as mobile entrepreneurship enablers (see http://www.infodev.org/mobilebusinessmodels for more information), focusing on the
creation of mobile software and applications.

                                                                                                                                 7
2. Is this Toolkit for You?
This toolkit was made for managers of tech hubs. Dozens of tech hubs have emerged over the last few
years across the globe. Managers of these innovation and entrepreneurship enablers grapple with
problems that infoDev has experience with. This toolkit takes lessons that infoDev has gathered from its
own tech hub pilots, mLabs and mHubs, and apply them to tech hubs in general.
The toolkit is especially useful for current and future mLab and mHub managers. mLabs and mHubs are
tech hubs established through grants administered by the infoDev Digital Entrepreneurship Program.2
infoDev is committed to supporting the analytical capacities of mLabs and mHubs. This toolkit is part of
that agenda. It will help grantees to improve local implementation while setting a common framework on
how to collaborate with infoDev on business analytics and performance measurements.
The third target audience is mobile innovation specialists at other World Bank units and other development
organizations, who design impact and measurement frameworks for tech hubs. Given the recent rise in
numbers of tech hubs, international development organizations are exploring if and how they can be
employed to achieve socio-economic development impact goals. In particular, tech hubs’ flexibility and
diverse potential effects have sparked interest but have also caused problems for specific and concrete
analysis and projection of hubs’ effects and impact. This toolkit addresses this complication. All elements
of the toolkit that speak of infoDev’s role in facilitating and coordinating with mLabs or mHubs on
business analytics processes can be seen as use cases with potential for replication and adaptation by
practitioners and decision makers of other development organizations, including relevant units of the
World Bank.




2   See http://www.infodev.org/workprogram.

                                                                                                           8
3. What Led infoDev to Develop this Toolkit?
Tech hub numbers are burgeoning in developing countries, helping information and communication
technology (ICT) developers and entrepreneurs to network, innovate, and start businesses. Set foot into a
top-tier hub and you will be struck by the buzz and excitement that have infused local entrepreneurial
communities within just a few years.
infoDev was at the forefront of the movement when, in 2011, it launched two different kinds of tech hubs
to enable entrepreneurship in local mobile application and software markets: mobile application labs
(mLabs) and mobile social networking hubs (mHubs). mLabs and mHubs were pilot mobile innovation
support programs. The immediate goal was to help infoDev learn from experimentation how the
innovation pioneer gap3 could be bridged through tech hubs. infoDev has since made great strides
learning lessons, making evaluations and publishing knowledge products.4 Each mLab and mHub operated
on different business models tailored to the needs of local markets, which increased the number of real-
world experiments that infoDev could learn from.




3 Auerswald, P. E. and L.M. Branscomb. 2003. “Valleys of Death and ‘Darwinian Seas:’ Financing the Invention to Innovation
Transition in the United States.” The Journal of Technology Transfer, 28 (3-4): 227-239; Baird, R., Bowles, L., and S. Lall. 2013.
“Bridging the “Pioneer Gap”: The Role of Accelerators in Launching High-Impact Enterprises.” Aspen Network of Development
Entrepreneurs and Village Capital.
4 See “The Business Models of mLabs and mHubs: An Evaluation of infoDev’s Mobile Innovation Support Pilots,” available at

http://www.infodev.org/mobilebusinessmodels; “Do mLabs Make a Difference? A Holistic Outcome Assessment of infoDev’s
Mobile Entrepreneurship Enablers,” available at http://www.infodev.org/mobile/mLaboutcomes; and case studies of mobile
entrepreneurs, available at http://wrld.bg/Ov4Vv.

                                                                                                                                     9
 Box 1: mLabs and mHubs as Local Nodes of the Digital Entrepreneurship Program’s Global Network
 mLabs are specialized mobile business incubation and acceleration facilities, offering physical
 workspaces, mentoring and coaching, devices for app testing, training, and start-up competitions.
 mHubs build mobile tech communities by convening a variety of stakeholder groups at informal
 gatherings, peer-learning sessions, conferences, and ideation and prototyping competitions.
 infoDev is motivated by a grassroots-oriented entrepreneurship support agenda. infoDev decided
 that the best way to stimulate technology innovation in developing countries is by giving operational
 independence to in-country grantees that implement mLabs and mHubs, and leave most of the
 implementation decision making to local partners.
 At the same time, infoDev provides technical assistance to mLabs and mHubs, leveraging its
 expertise, global partnership network, and unique positioning inside the World Bank for the grantees’
 benefit. So far, infoDev has helped mLab and mHub managers network at global conferences and
 published extensive evaluations and knowledge products on mobile innovation. infoDev found this
 knowledge to be relevant for stakeholders far beyond its network, and therefore makes products like
 this toolkit available to the public.




But the diversity of models is also a challenge when measuring the success of mLabs and mHubs
consistently and continuously. It was difficult standardizing comprehensive measurements of mLabs’ and
mHubs’ performance and effects. mLabs and mHubs themselves struggle to reliably track success
indicators over time. This problem is enhanced by the dynamic and complex nature of innovation
environments that mLabs and mHubs work in.
Altogether, this led infoDev to develop a measurement and analysis approach to codify its improved
understanding of how to capture mLabs’ and mHubs’ evolving effects. It also became clear that
performance and effect measurements had to be designed to be directly useful for mLabs and mHubs
themselves. infoDev’s grantees turn high-level concepts into on-the-ground reality. This also means that
they have the greatest insights into strengths and weaknesses of the tech hubs, and the most direct
access to qualitative and quantitative performance data.

                                                                                                         10
This Business Analytics Toolkit is thus an outcome of infoDev’s own learning. It codifies what the Digital
Entrepreneurship Program has learnt about performance measurement and business analytics, turning
these lessons into a practical guide for designers and implementers of tech hubs such as mLabs and
mHubs.




                                                                                                             11
4. Why Are Business Analytics Key to a Tech Hub’s Success?
Innovation and entrepreneurship support
                                                    Box 2: What are Business Analytics?
is a complicated endeavor. Entrepreneurs’
successes are determined by their                   In this toolkit, business analytics refers to the collection,
motivation, skills, and resources, but also         measurement, analysis, and sensemaking of quantitative
by complex dynamics of the innovation               and qualitative data that pertain to an organization’s
ecosystems in which they work.5 As a                performance and its effects on clients and stakeholders.
result, business analytics (see box 2 for a         The word business highlights infoDev’s belief that
definition) for tech hubs face particular           innovation support programs such as mLabs and mHubs
challenges and opportunities.                       should best be designed as value-creating, client-
                                                    oriented, flexible enterprises, whether funding is
    4.1. Using the “Build, Measure,                 provided by governments, international development
         Learn” Principle to Work in                agencies, or private sector investors.

         Complex Innovation
         Ecosystems
A foresighted, sound business analytics             Box 3: A Clear Statement from the Aspen Network for
approach is crucial for the success and             Development Entrepreneurs (ANDE)
sustainability of your tech hub. The
importance of measuring and analyzing               ANDE, in their widely noted 2013 report “Bridging the
                                                    ‘Pioneer Gap’”, found a clear lack of rigorous and
results is often underestimated, and is
                                                    sustained performance measurement among impact
often low on a tech hub manager’s long
                                                    accelerators; many of them tech hubs. This is in line with
list of priorities, especially during the set-      infoDev’s experience—in particular, ANDE’s observation
up stage. However, infoDev’s and others’            that client data is often tracked only for short periods of
experiences has clearly demonstrated that           time after they leave the program. This has prevented
there are great risks in keeping business           many hubs and accelerators from conducting reliable
analytics for later (see box 3). When data          assessments of their own impacts, as improved client
is only collected ad hoc, it becomes                success—both social impact and commercial success—is
difficult to understand the connection              usually their core goal. ANDE strongly recommends (1)
between provided services and their                 investing in tracking clients’ performances, (2) collecting
effects; detailed and continuous tracking is        and comparing data for successful and unsuccessful
usually more insightful. Consistency and            applicants, and (3) collecting data from clients for a
                                                    minimum of five years after they leave the support
continuity, even beyond a tech hub’s own
                                                    program.
clients and beyond the duration of support          Source:
programs, makes for the greatest possible           http://www.aspeninstitute.org/sites/default/files/content/docs/ande
learning from successes and failures.               /Bridging%20the%20Pioneer%20Gap%20The%20Role%20of%20Accel
                                                    erators%20in%20Launching%20High%20Impact%20Enterprises%20.p
What makes business analytics for tech              df
hubs difficult is that hubs shape local
innovation ecosystems and are influenced by them at the same time. They aim to have a positive effect on
various elements of an innovation ecosystem, ranging from direct improvements of client start-ups’
performances to broad changes in entrepreneurial culture. Tech hubs also design their service portfolios


5 See infoDev reports at http://www.infodev.org/mobilebusinessmodels and http://www.infodev.org/mLaboutcomes for more
information on the innovation ecosystem concept and case studies of ecosystems in which mLabs and mHubs operate.

                                                                                                                        12
so it can fill those gaps in the ecosystem that other organizations don’t fully address, so interaction
between hub and ecosystem goes both ways.
Innovation ecosystems in developing countries require support in many shapes and forms, and it can be
easy to lose focus. This often means that tech hubs struggle to clearly define their impact goals and
rationale. Still, like for any other organization, a clear vision and longer-term, strategic plan is essential for
tech hubs. You, the managers and designers, are being called upon to frame clear directions for your
hubs, while continuously tweaking business models and adjusting to changes in ecosystems. Resource
constraints and the need to coordinate with many diverse stakeholder groups add another layer of
complexity. The best tech hubs maintain the highest possible degree of flexibility without losing focus on
an overarching mission and strategy.
The “Build, Measure, Learn” approach—a key pillar of the Lean Startup philosophy--is useful in
understanding how business analytics can be embedded into iterative business model design. The
important insight is that organizational learning never ends, and that business analytics are not an
afterthought of implementation. Instead, learning and analytics are part of an iterative process that is at
the core of continuous and fast improvement (see figure 1). Another key argument is that start-ups
should not spend limited resources on developing elaborate, meticulous measurement and
documentation methodologies but instead measure a few simple and clear indicators at regular intervals.
Most emerging tech hubs are like to start-ups in that they too need to reconcile measurement rigor and
flexibility while maintaining small budgets.




                        Figure 1: Continuous improvement in the Build, Measure, Learn approach
The “Build, Measure, Learn” approach is particularly suitable for organizations that work to improve
innovation ecosystems. Simply put, the more complex and unknown the implementation environment,
the more difficult it is to make precise projections and plans. Recent ideas to improve lesson-learning in
international development have acknowledged this and begun to emphasize the need for piloting and
experimentation as well as failure detection and frequent and fast course corrections (or “pivoting” in
start-up parlance). For projects that operate in unknown and complex contexts, this is a superior way to
think ahead compared to rigid, static, and detailed long-term planning. Innovation ecosystems, with their
many diverse stakeholder groups—from government agencies to freelance software developers— can be


                                                                                                               13
understood as complex environments, so tech hubs stand to benefit from flexible, dynamic, and iterative
business analytics based on the “Build, Measure, Learn” idea.

     4.2. Six Ways for Tech Hubs to Benefit from Business Analytics
Sound business analytics will improve your tech hub in six essential ways (see figure 2):




                                                                              Showcasing
                                                                                clients
                                                                 Sharing
                             Being
                                                               success with
                          accountable
                                                                  clients
                                                    Learning
                                                      and
                                                   improving
                             Finding
                              focus                                                         Fundraising




Figure 2: Six motivations for business analytics

          4.2.1. Finding Focus for your Vision and Decision-making
Business analytics help you focus on what matters and make tough decisions to guide your tech hub
towards your vision. Tech hubs can pursue a wide range of value propositions and business models. The
list of services to choose from ranges all the way from hosting informal gatherings with a handful of
coders to managing an investment portfolio for gazelle start-ups. Opportunities and options are endless,
and different stakeholders will approach you for interesting projects that are not always an ideal fit. Tech
hub budgets are limited, and you will have to make tough decisions on how best to use your resources
and in which direction to take your tech hub. Only if you have clear targets for the core results that you
want to achieve will you be able to push essential services to maximum effectiveness and say “no” to ad
hoc activities that are irrelevant to your unique value proposition.

        4.2.2. Learning and Improving
Continuous learning about what works and what does not is a necessary condition for improving. You as
the manager or designer of a tech hub are in the best position to understand the hub. However, even the
best manager’s capacity is limited, and as the hub becomes larger and more complex, the less likely that
ad hoc lesson learning will be enough. You will also improve your joint decision-making with your
consortium members or advisory board if you can present them with well-documented information. If

                                                                                                          14
you want to continuously improve the value that you generate for your clients, you will need to measure
and analyze your performance as best as you can, much beyond your daily experience. This is particularly
true with tech hubs like mLabs and mHubs, which can have unanticipated, but important indirect
effects.(see box 4).

         4.2.3. Fundraising
You will need compelling analytics to fundraise and “sell” your tech hub to your funders, clients, and

 Box 4: Tech hubs with Far-Flung Effects: The Holistic mLab Outcome Assessment
 infoDev implemented the original four mLabs with quite broad implementation frameworks and
 setting only modest specific targets (see http://www.infodev.org/mobilebusinessmodels for more
                                                Box X: Using Infographics for Informative Storytelling
 background). Ultimately, mLabs turned out to have many indirect and systemic effects on various
                                                Infographics
 parts of local, regional, and international markets          have become
                                                       and innovation       extremely
                                                                       ecosystems.      popular
                                                                                    While        in recent
                                                                                           infoDev   had a
                                                years. They  are a useful way
 substantial amount of anecdotal evidence on these effects, towards the end of the grant period, it—
                                                                              to reach  a wider  audience   in
                                                                                                           was
 not clear what the holistic outcomes of mLabs  particular your funders—and communicate your journey
                                                   had been.
                                                and successes. Another function of infographics can be to
 As a response, infoDev conducted an outcome assessment (see
                                                use a storyline or narrative to explain and illustrate
 http://www.infodev.org/mobile/mlaboutcomes). The task at hand was not easy and even for a full
                                                complicated ideas in an appealing way. Both functions are
 research team of an external evaluation agency it was impossible to measure all of the mLabs’ far-
                                                clearly valuable for tech hubs as a new phenomenon
 flung effects. Still, the evaluation generated several useful insights. For instance, the assessment
                                                working in complex ecosystems. A few good examples
 showed that mLabs’ effects on start-ups’ business performance (revenues, investments raised, etc.)
                                                from infoDev and across the web are here, here, here,
 was actually just one—and probably not the most important—part of mLabs’ effects. Instead, mLabs’
                                                here, and here.
 catalytic effect as a broker of linkages in ecosystems and to some extent also their role in stimulating
 mobile app innovations with positive social impact was just as relevant an outcome as their immediate
 effects on the supported start-ups.
partners. Just like any other organization, you need to market your tech hub to potential clients, funders,
and other stakeholders. Your documented successes are the best argument to convince new client
entrepreneurs and developers to join. In tech hub business models, access to clients is often what is
relevant for partners and sponsors, so there should be synergies in your stakeholder management
strategy. It is a good strategy to combine storytelling and qualitative evidence with hard results numbers.
If you have a compelling and clear value proposition, and if you track important results numbers
consistently, it would be easy to summarize and frequently update your pitch in short brochures,
graphics, or videos. Infographics, as a tool to make complicated and dense information accessible to a
wider audience, have become particularly popular in recent years. You can also include a simple real-time
tracker that automatically imports selected statistics onto your website (see the website of Kenya’s
leading tech hub, the iHub, at http://www.ihub.co.ke/). Beyond informing readers, this helps to convey
immediacy and transparency of your website as a whole.

        4.2.4. Showcasing Clients
Analytics help to showcase your clients, giving them visibility and improving their chances of success. Your
own success as a tech-hub manager is tied to the success of your clients, whether you are leading a
community or incubating start-ups. Moreover, you play the role of a broker and marketer for your client
companies. Start-ups and early-stage entrepreneurs often do not have the resources for outreach and
partnership building, but tracking and marketing their stories will benefit both clients and yourself.


                                                                                                             15
Business analytics provide you with good arguments on the substance of your clients’ successes, making
your joint outreach and communication easier.

         4.2.5. Sharing Success with Clients
Your clients’ success determines your future revenue, and business analytics help you show them your
added value. Most tech hubs—especially those directly supporting start-ups—generate part of their
revenues directly from client entrepreneurs. Community members and co-creation space users might pay
a fee to attend special events or use desk space, and business analytics will help you show them what you
have to offer. Analytics are even more important to manage royalty and equity agreements with
incubation and acceleration clients. The issue here is that your revenue only starts to flow a long time
after you provide your services. For instance, an exit from an equity position might only make sense years
after the start-up has left the program and, also, royalty schemes may kick in only after the start-up
achieves a certain threshold turnover or profit. In these cases, long-term, reliable analytics will help you
make your case that you have actually added value to their businesses. Secondly, analytics record and
document when your clients received support, what kind of support, and the consequent results. This
makes interaction and relationship with your clients more transparent and creates accountability on both
sides, which will make it much easier to claim your fair share of your clients’ success even months and
years after you supported them.

         4.2.6. Being Accountable
If you receive funding from governments and donors, and if your tech hub is governed by a consortium,
business analytics makes it easier to fulfill additional accountability and reporting requirements. Public and
private donors in international
                                     Box 5: The Unexpected Benefits of Akirachix’ Reporting Struggle
development as well as
governments have a                   Akirachix is a grassroots community-builder for women entrepreneurs
responsibility to use their          in Nairobi. After receiving an mHub grant from infoDev, the small
funds judiciously, and to            organization grappled with the burden of reporting back on results,
ensure high levels of                struggling to free resources and staff to complete the task. The
accountability and                   challenges and feedback ultimately held good lessons for infoDev and
transparency. At the same            contributed to the motivation to develop this toolkit. But more
time, these organizations            importantly, the process also helped Akirachix formalize its results
often manage large budgets,          tracking and reporting structures, which contributed to its ability to
and the funding they provide         secure a further grant from the Swedish International Development
to tech hubs might only be a         Agency (SIDA). See http://www.infodev.org/mobilebusinessmodels for
small part of their overall          the full case study.
portfolio. As a result, they often require data to assess their own impact and accountability that might go
beyond the data that is immediately relevant for you. But with solid business analytics in place, it is much
easier for you to service your government and donor funder information requests. Similarly, your
consortium partners might be crucial supporters of your tech hub, but they too have an obligation to
ensure that they selected the right tech hub manager to run day-to-day operations, which they are
usually not intimately familiar with. Business analytics can help you illustrate and create documentation
of your work and its impact. Creating more extensive accountability and tracking structures might also
have positive side effects in professionalizing and formalizing your operations (see box 5).




                                                                                                            16
5. What Does Your Tech Hub Want to Achieve? Your Business Model as
   the Foundation for Business Analytics
Many different tech hub models are possible but a clear focus on your goals is necessary to set up a useful
business analytics approach. Tech hubs can fulfill various functions with various implementation models.
They can be profit-oriented or not, and their services can range all the way from hosting an informal
meeting with a handful of students to intensive multi-year, hands-on incubation programs. While diverse
services can complement each other and experimentation is vital, it is not possible to measure potential
effects that tech hub services might have. Instead, it is important to know why you are running your tech
hub in a certain way, and what your goals are. Having a very clear answer to this question will help
identify a small set of metrics that are relevant indicators of the effects that your tech hub has.
Once you have a clear goal, business model design is useful to understand how to align your services,
financial sustainability, and requirements of the innovation ecosystem. You can understand business
modeling as a structured way of thinking through your value proposition and how to improve and
monetize it. Your business model value proposition also links your service offering and operational model
to your overarching goal and vision for your hub. The value proposition is the benefit that achieving your
goal generates for your clients. So it is essential for you to understand how each activity that you
implement (with different partners, delivery channels, resources, etc.) contributes to increasing your
value proposition for your customer segments. Only when you have thought through this effect chain will
you be able to identify the most useful indicators for your business analytics approach. If you are
unfamiliar with business model design for tech hubs, several infoDev resources can give you further
information and guidance (see box 6).

Box 6: infoDev resources for tech hub business model design
The infoDev Digital Entrepreneurship Program has published several resources to help tech hub
managers and designers identify viable business models in the innovation ecosystems of developing
countries:
1. The Business Model Toolkit provides tools to develop and test a business model for an mLab or
   mHub, which can be adapted for other tech hubs.
2. The Business Model Evaluation report includes in-depth case studies of business models of four
   mLabs and three mHubs across five countries, as well as rich contextual and conceptual information.
3. The Holistic Outcomes Assessment of mLabs report takes a closer look at the effects that three of
   infoDev’s four mLabs have had, including their indirect and systemic effects on innovation
   ecosystems.




                                                                                                         17
In infoDev’s experience, tech hubs like mLabs and mHubs have three approaches to achieve their goals:

Box 7: Three business model approaches to support mobile application enterprises
Innovation ecosystems in developing countries often have gaps beyond the lack of support for start-
ups. For instance, innovator and entrepreneur communities could be fragmented, or the local talent
base might be too small to develop compelling technological solutions. As a result, infoDev found that
ecosystem building and skill development can be useful for longer-term and systemic improvement of
the conditions for mobile app start-up entrepreneurs. The three different approaches can be
connected back to groups of services that mLabs and mHubs can offer.
Start-up Creation: mentoring, seed funding, focused networking (including with investors),
acceleration-type start-up competitions, deal brokerage, marketing support, office space, business
support.
Skill Development: technical and business training, workshops, app testing facilities, prototyping events
and hackathons, coaching, virtual learning.
Network Building: informal networking events, multi-stakeholder conferences, virtual community
building, team-building and ideation competitions (hackathons, start-up weekends, barcamps).
                        Ultimate goal: Support growth-oriented mobile entrepreneurship.
 How? Three different but complementary approaches
 1. Start-up Creators                   2. Skill Developers                   3. Network Builders
 Start and develop new growth-          Broaden entrepreneurial and           Bring together diverse
 oriented start-ups.                    technical talent pool and train       stakeholder groups and help
                                        competent potential start-up          activate and organize
                                        founders and employees.               communities.


 Which services? Business models typically focus on different services
 Informal networking events and         Technical and business trainings,     Regular, in-depth, one-on-one
 gatherings                             workshops, and clinics                start-up mentorship and coaching
 Multi-stakeholder conferences,         Virtual learning courses and          Core business support
 often including small-scale            platforms                             (accounting, financial
 innovation competitions (example,      Mobile app testing facilities.        management, legal services, etc.)
 start-up weekends, barcamps)                                                 Business development: brokerage
 Blogging, newsletters, and mailing                                           and mediation of formal
 lists.                                                                       contracts, grants, and
                                                                              partnerships.
 Questions to ask yourself:
 Which approach is the best fit for you and your innovation ecosystem? All of them, one, none?
 If you have limitations in one of the areas, are there other partners that you can engage to help?
 If you are well suited to deliver all three, what resources will you need to effectively implement, monitor, and
 report?




                                                                                                                    18
start-up creation, skill development, and network building. Every business model is different, but there are
categories of tech hub business models that can help narrow down your business analytics. For instance,
it will help determine metrics if you are clear about what you are interested in: sowing the seeds of
entrepreneurship at the foundations of the ecosystem, or work with start-ups and help them become
regional and global leaders. infoDev has identified three different approaches that support growth-
oriented mobile entrepreneurship but tackle gaps in ecosystems in distinct ways: Start-up Generators
have start-up creation and development as their direct goal; Skill Developers focus on broadening the
entrepreneurial and technical talent pool and train competent potential start-up founders and
employees; and Network Builders bring together diverse stakeholder groups and help activate and
organize communities (see box 7 for examples of services that these three approaches typically entail).
These three approaches are not mutually exclusive, but if you know which approach you want to take, you
will have a better idea which business analytics to use. Clearly, Skill Development, Network Building, and
direct Start-up Creation are all important for mobile app enterprises to emerge and thrive. It is definitely
advantageous to pursue all three at the same time, and there could be important complementarities and
synergies.6 Some services, like innovation competitions, may play a role in all three approaches. However,
to develop a viable and compelling business analytics approach, it is important to know in which direction
you want to go and which identity you want to have.


For example, there could be synergies, but there also could be tradeoffs and tensions between different
services, as all tech hubs have limited resources. Business analytics are important in this context because,
if your targets for example revolve around Start-up Creation, you could abandon informal networking
events when push comes to shove. But if your goal is community building within a Network Building
mindset, and if your tracking and targets relfect this mission, you could neglect services for Start-up
Creation such as hands-on mentoring and seed financing. In other words, business analytics are
determined by your goals and clear vision, but over time they also affect the services you implement and
the tradeoff decisions you make. Setting the right course and being aware of differences between
approaches is therefore important before you start to design specific indicators or performance-
measurement systems.




6   See the case study and lessons sections in http://www.infodev.org/mobilebusinessmodels.

                                                                                                          19
6. Who are Your Funders? The Special Case of Funding from Donors and
   Governments
If you receive funding from donors7 or government organizations, you need to think beyond the value
proposition for your immediate clients. If you are a profit-oriented tech hub that generates all of its
funding directly from clients, identifying your value proposition within your overarching goal and business
model framework provides you with a sound basis to understand which metrics matter for business
analytics. But if your funders provide you with resources so that you create value for someone else, you
will also need to understand the funders’ own success metrics.
Tech hubs often have several different public and private funders and sponsors. It is important to find
common ground between the goals of all funders and of the hub itself. Funding rarely comes without
strings attached, and not all funding will allow you to directly work towards your own long-term goal. If
you have diverse partners from government agencies, to multinationals, to grassroots organizations, you
need to be sensitive to their respective goals and their success metrics to be able to identify the best
possible business analytics approach for your hub.
The basic alignment between your funders’ goals and your own should come from the value proposition for
your clients—this is what ultimately determines your funders’ impact. Governments and donors are not
your clients, as you do not provide value directly for them. Their reason for funding you is that they
believe that you are better positioned than they are to provide value to someone else, “beneficiaries” in
international development language—in this case, early-stage tech entrepreneurs and start-ups. In other
words, by providing value to your clients, you help your funders have a positive impact on others’ lives,
instead of generating profit or utility for themselves.
Governments and donors have their own vision and logic on how activities lead to impact, which they often
codify in an impact model or a “theory of change.” For instance, a donor might conceptualize that business
mentoring for young software developers leads them to start and run sustainable and growth-oriented
businesses, which means that more people are employed and more taxes paid than without the
mentoring. So, mentoring ultimately contributes to socio-economic development and improved quality of
life. In infoDev’s case, the envisioned impact is related to private sector development, so the focus is on
metrics such as job creation and revenue generation through start-ups (see box 8).




7 Donors is defined loosely and includes all organizations that contribute funds to a tech hub, mLab, or mHub with the
expectation that it creates value for someone other than themselves. Examples include foundations, philanthropists,
international development organizations, NGOs, and other grant-making organizations.

                                                                                                                         20
                                                               If you want your business analytics to fit in
 Box 8: A success on many levels: the case of Kopo Kopo
                                                               with your funders’ impact models, you
  One of mLab East Africa’s most successful incubation         should work with them to identify existing
  graduates is the mobile financial services company Kopo      and potential overlaps, and do so
  Kopo. In just a few years of operation, the enterprise       proactively and continuously. Many
  created about 50 jobs and served more than 10,000 small      problems that grant recipients have with
  business customers. In 2013, Kopo Kopo secured series A      reporting through scorecards and
  funding of $2.6 million. Kopo Kopo attributes substantial    logframes stem from the underlying
  value to the support that it received from the mLab in its   problem that funders can have
  early days (see http://www.infodev.org/mobile-               somewhat different perspectives and
  entrepreneur-case-studies).                                  priorities. They might use a different
  For mLab East Africa, Kopo Kopo greatly contributed to its   language, not fully aware of your
  targets to increase revenues and investments of              constraints and local realities. Donor and
  incubatees. For infoDev and the Digital Entrepreneurship     government funders also tend to request
  Program, both the job creation effect and Kopo Kopo’s        as much information as they can to
  function as a role model and active member of the            assess any potential impact that their
  Nairobi innovation ecosystem were important private          funding has generated, which can put
  sector development results. Finally, for infoDev’s donors,   pressure on you, given your resource
  such as the government of Finland, Kopo Kopo represents      constraints and narrower focus. Your
  an example how growth-oriented mobile application            funders are also held to higher-level
  enterprises broadly contribute to socio-economic             impact and results agreements that they
  development in low-income countries, in this case by         made with their own funders or
  increasing financial inclusion of small businesses and       governing bodies. In this complicated and
  transformative impact on payments.                           indirect communication and codification
                                                               chain, overlaps might not be obvious and
tracking and evaluation might become more cumbersome than it should be. It will help if you engage in a
dialogue with your funder to continuously understand their requirements and communicate your own.
Ideally, impact models and your value proposition are interlocked in a coherent logic. Usually, donors and
governments fund an array of projects like yours, and your project is only part of their program portfolio.
This means that your performance feeds into your funders’ activities that are conceptualized to lead to
some type of wider-scale impact. Ideally, the impact models of higher-level programs are aligned, and the
value you create contributes to the start of your funders’ results chains (see figure 3). While it is not your
responsibility to be intimately familiar with all higher-level impact models, general awareness of how your
results are interlocked with this can help you to better communicate with your funder and work towards
better alignment, or think about solutions when alignment cannot be reached.




                                                                                                           21
Figure 3: Alignment of a tech hub business model with funders’ impact models with infoDev as an example.
Source: Catherine Amelink.


Your government or donor funder can often be a valuable source of information to decide your business
analytics approach. The fact that your government and donor funders usually manage several projects like
yours can help you conduct benchmarking and comparisons with other tech hubs. For instance, infoDev
brought together mLab and mHub managers from across the globe about once per year and, at the end
of the grant period, codified lessons in several reports (see box 6). Another example is the effort of the
German Agency for International Cooperation (GIZ) to work with Afrilabs. Together, they organized two
annual reunions in Berlin that helped African hub managers to coordinate and strategize a joint agenda.8
Knowledge-oriented international organizations are also increasingly publishing rich reports on
innovation ecosystems in developing countries. While direct comparisons of concrete results figures (for
example, number of start-ups created) can lead to unfair comparisons that neglect the ecosystem
context, most of these resources use both qualitative and quantitative evidence that you can use as
“enriched benchmarks” and learning material. Examples of resources are included in the appendix.




8   See http://www.giz.de/en/mediacenter/11922.html and http://re-publica.de/en/session/opening-global-innovation-lounge.

                                                                                                                            22
7. What Should You Measure? Developing a Business Analytics Approach
   and Selecting Indicators
What you should measure should be solely driven by your goals and your specific context. As explained in
the previous sections, your own goals, your business model (adjusted to the local context), and your
funders all help decide the right approach to business analytics. Tech hubs are not all the same, and so
there is no common recipe. That is true even among infoDev’s tech hubs, mLabs and mHubs. Each mLab
and mHub has different local co-founders and different local ecosystem conditions that they address. At
the same time, mLabs and mHubs are initially infoDev grantees, which means that they must be aligned
and contribute to infoDev’s Digital Entrepreneurship Program impact model. While they may have
additional more confined goals with regard to supporting mobile innovation ecosystems and mobile app
entrepreneurs, the impact model is a tested one and its indicators are based on previous experience in a
variety of countries and unique ecosystems. Grantees also receive guidance and inputs from infoDev
throughout the process. In the following section, the toolkit will draw from infoDev’s past experience with
mLabs and mHubs and use their examples to illustrate potential approaches for tech hubs in general.
While the general logic remains the same, you will need to adapt the guidelines provided here if you are
dealing with different funders and different overarching impact models.

    7.1. General Guidelines for Tech Hub Performance Indicator Selection
You should always remind yourself of the general principles of business analytics, as this will help you to
avoid common mistakes. In infoDev’s experience, tech hubs and incubators often struggle to follow
general principles of performance measurement and evaluation (see figure), either because they lack
awareness or because they do not prioritize this agenda early on, only to feel the consequences later.




                                                                                                              23
                                                                 Less can be
                                                                    More




                              Inform your                                                         Focus on your
                              stakeholders                                                         contribution




                                                                Tips for
                                                               indicator
                    Balance flexible                           selection                                 Longitudinaland
                                                                                                           before/after
                    and consistent
                                                                                                               data




                                             Qualitative for
                                                complex,                       Quantitative for
                                             uncertain, and                      measurable
                                                 context                        value creation
                                              information




         7.1.1. Less Can Be More
There is often a temptation to measure everything that can be measured. More data is often confused
with more information and, therefore, more knowledge. The greater the number of data points one has,
the greater the capacity needed to make sense of it; and the greater the complexity in understanding
what is really important to your specific mission. Further, as implementation is what we do every day, the
majority of indicators tend to be developed for the output level, with a lack of attention to the measures
that really matter in the long term, outcome indicators. Outcome indicators measure your added value. If
you cannot demonstrate this value, the position of your tech hub becomes much more tenuous. For
instance, the number of people in a contact data base or the number of people attending a prototyping
competition can be meaningful if your goal is to reach a lot of stakeholders, but if your goal is to create
start-ups, the link between these data points and your success is less clear (see box 9).




                                                                                                                           24
 Box 9: Can Hackathons Create Start-ups?
 Ideation and prototyping competitions have become popular in recent years. The notion that
 hackathons create a large number of functional mobile app innovations in a very short time is
 compelling. For business analytics, there is a temptation to count the number of innovators attending
 hackathons and the number of prototypes to infer that many innovations have been created.
 But infoDev’s and others’ experience has clearly shown that hackathons by themselves rarely create
 sustainable app innovations or viable start-ups. Instead, based on early lessons, mLabs, mHubs, and
 infoDev have quickly shifted towards innovation competitions that emphasize the depth of coaching
 and mentoring, including strong event preparation and follow-on support. While this means that
 fewer teams and individuals can be supported, the impact on start-up creation and success can be
 much higher. Such effects, however, have to be tracked over time by following the paths of
 innovation competition winners and those that did not win a prize. The amount of revenue they make
 and how much it increases might be a better measure for the success of a competition. See
 http://www.infodev.org/m2work and http://www.infodev.org/mobilebusinessmodels for more
 information.
Another problem with measuring too much is that your stakeholders will get tired of being evaluated,
especially if you rely on surveys and interviews. You should ultimately arrive at a clean, clear, short list of
qualitative and quantitative indicators that you want to measure, which should only contain those
indicators that are definitely relevant for the impact you want to have and the value you want to provide.

          7.1.2. Focus on Your Contribution
The standard approach to impact and performance evaluation is to do comparisons between a group of
people that has used or benefited from the offered service (the treatment group) and a group of people
that was similar at the outset but did not receive the service or support (the control group). The success
that the treatment group has above and beyond the success
of the control group can be claimed to be the contribution.         You should ask yourself:
This is similar to conducting real-world experiments. The           Where would my clients be without
logic is similar to evaluation approaches such as randomized        receiving the value that I provided for
control trials of health care interventions or A/B tests in         them?
(online) marketing.                                                 What are my clients doing differently as a
However, it is extremely difficult to find control groups for           result of the services (the value) I have
the kinds of services that mHubs and mLabs offer.                       provided to them?
Stakeholders of innovation ecosystems are difficult to
compare as they depend on each other in complex ways. It becomes tough to isolate a tech hub’s
contribution effect without substantial effort. While you should keep in mind your contribution to your
clients’ success, all of your clients’ success is not a result of your support. Simply put, you should ask
yourself: Where would my client be without the value that I have provided for them?

       7.1.3. Longitudinal and Before/After Data Is Key
The best alternative to a proper contribution analysis is to track data for the supported group over
extended periods of time and make inferences about changes to it that were very likely due to the service
you provided. For instance, you will know that your support was useful if a small start-up joins your
program, having had flat revenues for a year or so, and three months after joining, its revenue increases

                                                                                                                    25
by 30 percent. This also works for specific services within your portfolio: you can compare revenue
evolution during the year before a new mentor was recruited and over the following year.
The same is true with perceptual and qualitative indicators such as satisfaction with your service that
clients express in surveys and interviews. It is essential that you consistently and rigorously track key
indicators and collect additional contextual information, which should be easier once you identify a short
list of indicators as described in the first principle.

         7.1.4. Use Quantitative Information but Target It Wisely
Numbers are a powerful communicator, and almost every business analytics approach will include some
sort of quantitative element. However, making quantitative data reliable and meaningful is often not as
easy as it seems. For instance, the number of people joining your events might give you a good idea of
your reach in the local ecosystem, but it does not tell you much about the quality of connections that
participants made during the event and how this changed their success or the success of your immediate
clients. Quantitative data collection was also cumbersome for mLabs and mHubs because their activities
and services change frequently depending on ecosystem needs, and a lot of effects happen through
informal exchanges and through indirect paths.9
So, you should ask yourself how the value you provide can be quantified meaningfully before you look at
any data points just because they are easily available. What is the best proxy measure for the kind of
value that you want to achieve? This exercise goes hand in hand with the first and second principle: you
should find a handful of quantitative indicators that directly speak to your value proposition. Focus on
those, tracking them consistently.

         7.1.5. Don’t Underestimate the Qualitative
Quantitative data will always be part of your business analytics, but they will rarely be enough for you to
understand and improve your performance and value proposition. This is true because many of the
effects of services that mLabs and mHubs provide unfold only over time and in indirect ways. You are
trying to affect different elements of a complex innovation ecosystem, so some things will remain
uncertain and impossible for you to fully capture and quantify.
This is where qualitative data can be powerful.10 First, it can help you to get a rough idea of the
magnitude of your contribution: if your clients express that they could not have achieved what they did
without your support, this will give you a clear indication that your program provides value—which is also
why testimonials are generally such a powerful communicator.
Second, people’s perceptions drive their actions. You can also use qualitative evidence to understand the
effects of your support on your clients’ and other ecosystem stakeholders’ changing beliefs and
motivations. For instance, local innovation ecosystems are in large part driven by buzz and excitement,
and community building in particular relies on individuals’ vision and drive. So if you can reliably track, for
instance, that your clients’ confidence to start a business or lead a community has changed as a result of
your support, this is evidence of a valuable contribution that you would not be able to capture with
quantitative data alone.

9 This was confirmed in infoDev’s mLab Outcome Assessment, which found that mLabs’ ecosystem impact is indirect and
complex, but nevertheless palpable. See http://www.infodev.org/mobile/mLaboutcomes.
10 This is also why infoDev emphasized qualitative evidence for its initial evaluations of mLab and mHub pilots implemented

under the CSBKE program, see http://www.infodev.org/mobilebusinessmodels and
http://www.infodev.org/mobile/mLaboutcomes.

                                                                                                                              26
Third, other lessons and learnings, which are strongly context and situation-related, could be difficult to
capture with quantitative indicators. For example, a certain mentor with a strong personality could work
well for some of your clients and not so well for others, or a political incident could affect the ecosystem.
Such information, very important to understand your clients’ success, is unlikely to be uncovered through
quantitative data.

        7.1.6. Find the Right Mix Between Consistent and Flexible Measurement
Simple quantitative indicators provide most insight when measured consistently over extended periods of
time. On the other hand, as an mLab or mHub that is starting out, you will almost definitely adjust your
business model, or you might even pivot to an entirely different model, which will of course affect your
choice of performance metrics. Overall, the mix between consistency and flexibility also means that the
indicator selection process will be reiterated over time—very much in the spirit of the “Build, Measure,
Learn” principle— and infoDev and you will need to flexibly adjust whenever this helps you better
understand your value contribution and impact.
Altogether, this means that you will need to be consistent and flexible at the same time. In other words,
you should differentiate between those key (quantitative) indicators that are most insightful when
tracked continuously and consistently, and those where you can flexibly adapt without losing too many
insights as you adjust your business model. This point emphasizes even more that the handful of key
quantitative performance indicators that you want to track should be chosen carefully, as you will benefit
from sticking with them over a long time period. Qualitative indicators, on the other hand, lend
themselves more to adjustments; you can more easily incorporate additional contextual and exploratory
information queries, for instance, taking interview questions in a different direction once you uncover
something new or surprising.

         7.1.7. Integrate Performance Tracking in Agreements with Your Clients and Partners
A big challenge with performance measurement is that there is wide agreement that it is relevant and
useful, but often the people that hold the critical information (in particular, clients and partners) are
reluctant to spend time and effort to share it. There is often a fundamental misalignment of incentives:
your clients and partners might think that the information is relevant only to you and not to them, and
they gain nothing by spending time sharing it with you.
This means that you need to go the extra mile to convince your clients and partners that your
performance tracking will benefit them as well. It also implies that you have an obligation to keep
information seeking from clients and partners to the necessary minimum. This is in line with the principle
that you must only focus on a few key indicators and consistently track them. Lastly, you should also
share the insights that you derive with your clients and partners, at least as far as they are relevant to
their work. There is a chronic dearth of information in many if not most innovation ecosystems in
developing countries, so your clients and partners are likely to appreciate your evaluation efforts if you
show them the knowledge benefit that they get from you when they share the information you need.
It will also help if you discuss your performance measurement requirements with clients and partners
early on. You should include specific clauses and requirements in formal support agreements that you set
up with your start-up and entrepreneur clients. This will help them anticipate the time and effort that
they are expected to invest in providing you with feedback and information on their progress. In
particular if your business model relies on revenue sharing or royalties from start-ups and entrepreneurs,


                                                                                                            27
you will need to formally agree with your clients on reporting requirements even beyond the duration of
their participation in your program.

    7.2. Selecting Indicators and Performance Metrics for Tech Hubs

         7.2.1. Funder-Designed vs. Your Own Business Model
There are two scenarios for setting indicators and
                                                                    If you can measure it, you can manage
performance metrics for a tech hub: either your funder
                                                                    it. The indicators you select are your
proposes a comprehensive business model including indicator
                                                                    guideposts; providing you with key
templates and measurement plans, or you as the aspiring tech
                                                                    progress information and the ability to
hub-lead develop the business model and indicators yourself.
                                                                    correct course if performance is not
At the outset, you need to assess the basic requirements that
                                                                    what it should be.
your funder sets. For instance, infoDev has experimented
with different approaches to business model design and
indicator selection for its tech hubs, depending on the project context (see box 10). Your funder may give
                                                         you a specific implementation and business
 Box 10: infoDev’s Role in Business Model Design         model, including indicators, or if they may expect
 and Indicator Pre-Selection                             you to develop the business model and indicators
                                                         from scratch.
 When setting up the 12 original mLabs and
 mHubs, infoDev asked for proposals of business          If you design your own tech hub business model,
 and implementation models from local grant              identifying performance metrics should be part of
 applicants who were given only a relatively broad       your business model design before you pitch the
 framework of expected deliverables, services, and       model to your funder. infoDev has published the
 functions. infoDev chose a different approach for       Business Model Toolkit for mLabs, which takes you
 the Caribbean Mobile Innovation Project (CMIP).         through all the steps necessary to design a
 Here, it involved the region’s mobile innovation        comprehensive strategy and value-creation
 stakeholders in a design process and then               approach for a tech hub that can be tried out in
 developed a detailed business plan. The plan            the market. The following presupposes that you
 would guide implementation of services and also         have run through this exercise and already have a
 specify the business model for the grantee over         good idea of the market conditions and your
 the four years of the project. This approach was        goals, strengths, and weaknesses.
chosen as infoDev was well positioned to provide       If you advance in your application for a tech hub
a birds-eye view of the Caribbean project, where       grant, you will usually co-develop your
nascent innovation communities on different            performance metrics with your funder. As you
islands, with different backgrounds and economic       progress through the stages of a tech hub funding
contexts, had to be organized in a coherent large-     application process, you will collaborate more and
scale project. For future mLabs and mHubs,             more with your funder to further develop your
infoDev will determine the depth for upfront           business model, including your business analytics
business planning and indicator selection on a         approach. For instance, for an infoDev mLab or
case by case basis.                                    mHub grant, you might suggest a crude list of
                                                       indicators during the expression of interest phase,
                                                       and then advance with detailed indicator lists,
definitions, and targets based on the feedback received from infoDev when you submit your application



                                                                                                        28
to the call for proposals. During this process, you should see your funder as your partner, and both parties
will benefit if you are transparent about any concerns or constraints that you anticipate.
The co-development process should be interactive, but your funder will usually have some basic
requirements. As described in section 6, your value proposition will need to fit in with your funder’s
impact model. In the example case of the Digital Entrepreneurship Program, the overarching goal is to
find, nurture, and help accelerate high-growth potential mobile applications enterprises. Many services
and activities that tackle early-stage innovation gaps are possible under this umbrella—from community-
building and ideation events to intense accelerator programs that emphasize one-on-one business
mentoring and equity investments. At the same time, the program’s focus on growth-oriented
entrepreneurship clearly frames the metrics and evaluation approaches that are relevant to assess its
effects, and suitable mLab or mHub business models will usually pursue one or a combination of the
three business model approaches of Network Building, Skill Development, and Start-up Creation (see box
7).

         7.2.2. Implementation Quality/Output Indicators
You will need to track a set of core implementation indicators, measuring to ensure that your tech hub is on
par for service efficiency, effectiveness, and quality. There are several basic measures that indicate
whether your tech hub is functioning at a good standard. This list of measures should be easy to track as
it relates mostly to basic records of your activities and tracked client participation. In an impact model,
these measures would be called output indicators, which reflect your funder’s direct accountability
towards their own donors. Mostly, these indicators do not by themselves imply that you are providing
value for your clients: instead they track that you are implementing of the services, rather than their
effects on the clients. As an example, table 1 summarizes the output indicators that infoDev usually
expects mLabs and mHubs to track. Even though these indicators seem basic, infoDev requires mLabs and
mHubs to keep consistent records and report back at regular intervals. This will be true for other funders
too. The appendix includes detailed definitions of some the indicators.
   Getting the mLab or mHub up and         Building capacity to deliver             Delivering services
                running
 Business/implementation              Number of partnerships with           Number of
 model/plan developed/co-             financial and non-financial service   businesses/entrepreneurs receiving
 developed and revised with infoDev   providers.                            services: different levels/kinds of
 (infoDev template or own)                                                  support, increases in the volume of
                                                                            services over time.
 Number and type of                   Amount and type of financial and      Volume of seed funding from mLab
 consortia/partnerships formalized    non-financial contributions (human,   or mHub received by
 to deliver the                       in kind) secured/committed from       businesses/entrepreneurs (if
 business/implementation              partners for additional support of    applicable)
 model/plan                           businesses/entrepreneurs
                                      (equivalent to resource leverage by
                                      mLab or mHub, unless
                                      implemented services uniquely
                                      benefit non-entrepreneurial
                                      stakeholders)
 Development of sound and robust      Institutional capacity established    Number of
 governance structures for the mLab   (for example, functional reporting    businesses/entrepreneurs reporting
 or mHub (for example, advisory       and governance processes)             satisfied or very satisfied with
 board set up)                                                              mLab/mHub services



                                                                                                              29
     Locally relevant results framework   mLab or mHub manager/staff’s use       Number of knowledge sharing
     and performance monitoring           of gained implementation               events and knowledge products
     frameworks developed and in use      knowledge.                             developed.
     by mLab or mHub to collect and
     share data and lessons.
     Institutional capacity established   mLab or mHub manager/staff’s           Number of media appearances
                                          changes in implementation capacity
                                          (self-rated)
                                                                                 Enabler cost to sustainable revenue
                                                                                 ratio


             7.2.3. Value Proposition/Outcome Indicators
    In addition to implementation quality indicators, you will need to track a set of core value
    proposition/outcome indicators. Beyond your basic implementation quality, you will need to assess to
    what extent you succeed in offering the value proposition that you envisioned for your clients. In mLabs
    and mHubs, managers must track a short list of indicators that assess the effectiveness of their
    implementation, that is, the value created and the outcomes they contribute to.
    infoDev has identified three separate approaches to mobile app enterprise support for tech hubs: Start-up
    Creation, Skill Development, and Network Building. Tech hubs can pick from an extensive list of services
    when they design their business model. But the Digital Entrepreneurship Program’s experience with the
    mLab and mHub pilots has shown that some services have the potential to support mobile app
    enterprises directly while others rather serve to set the foundations for enterprise development in
    ecosystems with large gaps that prevent start-ups from emerging. Skill Development and Network
    Building were identified as crucial approaches, aside from direct Start-up Creation (see section 5. and box
    7).
    The core value proposition/outcome indicators that you will need to track depend on the approach that you
    pursue. infoDev’s past experience has shown that for each approach—Start-up Creation, Skill
    Development, or Network Building—different sets of core indicators are needed to measure outcomes.
    For instance, trainings implemented within a Skill Developer model might have substantial effects on
    mobile app entrepreneurship, but it might be impossible to link the indirect and long-term effects of
    trainings to typical Start-up Creation outcome indicators, such as increases in start-up revenues. The core
    lists of possible indicators are deliberately kept short, and you should be able to choose additional
    indicators that allow flexibility in your business analytics approach. The appendix includes detailed
    definitions of the Start-up Creator indicators.
     Possible indicators for a Start-up     Possible indicators for a Skill         Possible indicators for a
                  Creator                            Developer                         Network Builder
-    Additional sales revenue for         Increase in skill level of            Usefulness and outcomes of
     businesses                           businesses/entrepreneurs (self-       networking and community-
                                          reported, for example, measured       building events as reported by
                                          in tests and through surveys)         participants (for example,
                                                                                measured through twice annual
                                                                                surveys)
-    Profitability of businesses,         Increase in investment readiness of   Number of registered/signed-up
     including changes over time          businesses/entrepreneurs              community members as a result of
                                          (baseline measure at entrance and     the activity of the mLab or mHub


                                                                                                                   30
                                              measure at exit, based on
                                              tool/scorecard, measures change
                                              against criteria at exit)
-    Number of businesses who raised          Jobs/salaries/contracts obtained as   Increase in the linkages between
     grants, early or growth stage            result of the services as reported    organizations in the ecosystem as a
     finance, including amount of             by businesses/entrepreneurs (for      result of the activity of the mLab or
     financing                                example, measured through             mHub
                                              surveys 6 months, 12 months, and
                                              24 months after the support)

     Number of digital                        Number of digital product/service
     products/services                        prototypes developed/improved
     developed/improved
     Number of users reached by
     developed/improved digital
     products/services
     Number of new new direct jobs
     created by businesses


    There could be additional indicators that your funders require you to track for specific service agreements
    or projects. The social impact potential of mobile applications is widely recognized, and international
    development organizations and institutional development funders are exploring approaches to stimulate
    inclusive and social mobile app innovations. This means that they might integrate additional indicators in
    their impact models to incentivize you to develop your services in a ways that they have greater potential
    to generate a specific type of social impact. For instance, infoDev together with the four mLabs and mHub
    Nepal ran the global mobile microwork challenge “m2Work,”11 and many mLabs and mHubs have
    individually attracted social impact funding or run inclusive innovation competitions (see box 11).
    Similarly, strengthening opportunities for women to close the gender gap in tech entrepreneurship is


    Box 11: mLabs Tapping into Entrepreneurial Communities to Generate Social Impact
    A good example of a social impact innovation competition is mLab East Asia’s and UNICEF’s Mobile
    Hackathon in June 2013. The mLab tapped into its large network of entrepreneurs and coders across
    Vietnam to promote developing innovations for children. Close to 100 hackers gathered at the main
    event in Ho Chi Minh City; two teams won the challenges that UNICEF had set out and received cash
    prizes (see http://www.unicef.org/vietnam/media_21108.html). The mLab also provided follow-on
    support for the best teams.
    In January 2014, mLab East Africa embedded a social enterprise track into its incubation program.
    The mLab set up the Mobile Impact Ventures Program (see http://mlab.co.ke/mivp/) in partnership
    with the Global Impact Investment Network and with financial support from the Rockefeller
    Foundation and Tony Elumelu Foundation. Focusing on the agriculture, education, and health and
    water sectors, selected start-ups undergo a three-month intense mentoring program and group
    trainings. The venture with the most traction will then receive seed funding of $5,000 and support
    to attract additional growth capital.


    11   See http://www.infodev.org/m2work.

                                                                                                                            31
important for infoDev, and it has started to develop a women entrepreneurship agenda.12 infoDev will
facilitate and sometimes integrate such initiatives into its core programming in collaboration with donor
partners. In cases where infoDev provides you with funding to specifically implement services targeted at
social and inclusive innovation, additional outcome measurement will become necessary. Given the broad
variety of potential initiatives and focus areas, it is impossible to specify these indicators upfront, but
infoDev and other funders will engage with you to develop a viable and useful indicator approach on a
case by case basis.




12
     See http://www.infodev.org/workprogram.

                                                                                                        32
8. How Should You Measure? Putting Your Business Analytics Framework
   into Action
Once you select your performance indicators, you will need a measurement approach that is feasible and
blends smoothly with your roll-out. The best indicator selection is useless if you cannot track them reliably
throughout implementation. Given that tech hubs are so diverse and usually implement a wide array of
services, putting measurement into practice can be where things go wrong. For instance, it might look
realistic and useful on paper to collect a large number of indicators, but then some metrics might turn out
to be difficult or impossible to track, there can be evaluation fatigue and low response rates from clients
and partners, or the collected information might be less useful than originally envisioned. Your business
model, the agreement you have with infoDev, and feasibility conditions that you face in your local context
will always determine the best approach to put your business analytics into action. This section will
introduce you to the basics.

    8.1. Establishing a Performance Measurement System
You should set up a performance
measurement system that is a master           Box 12: The CMIP Performance Measurement System
repository and documentation of all           The Caribbean Mobile Innovation Project (CMIP) was
indicators that you track and all analyses    designed to foster entrepreneurship and the start-ups
you conduct. A performance                    ecosystem with a region-wide approach, nurturing early
measurement system is at the heart of         stage mobile app innovators across the Caribbean
any business analytics approach. Over         islands. Given the complexity of the project, infoDev
time, the system also becomes a               decided to conduct substantial feasibility scoping and
resource for you to detect effects of         business planning upfront. In 2014, infoDev gave a grant
larger trends in the ecosystem or results     to the UWI consortium that covered a large part of the
that change due to strategic decisions        cost of the project for four years. infoDev, together with
you make. The system will make all            the Canadian government, also developed a framework
(codifiable) analytics available to you       of key performance indicators (KPIs) that would guide
and others to whom you give access.           the CMIP’s priorities. The KPIs firmly set the consortium’s
You can see this as your performance          focus on project sustainability, setting expectations of
and value creation data base, which you       increasing effectiveness and co-financing.
fill with relevant and increasingly
insightful bits of data, information, and
analysis.
The main function of the performance measurement system is to provide you with a consistent structure
that allows you to codify your findings and measurements. Ideally, the performance measurement system
will become your one-stop shop for all information that is relevant to understand your performance and
the value you provide to your clients. Through the logical links that exist between implementation
quality/output indicators and value proposition/outcome indicators, the system will show the areas in
which you are on track and in which you are not. Over time, the system will also allow you to set targets
more realistically. It will help you maintain consistency and will become your frame of reference for all
things related to performance and business analytics during the rollout and operations of your tech hub.
Keep it simple: Advanced software programs and intricate quantitative indicators are not always necessary.
The software market is flooded with software solutions that are meant to help you track and document

                                                                                                            33
your indicators, with software packages including more and less sophisticated analytical functions. While
good software can definitely be useful, you need to find out for yourself what level of complexity is most
useful for you and your staff. The challenge will be that tech hubs are not typical businesses, specialized
digital performance tracking tools and interfaces do not yet exist. Moreover, learning to use and
maintaining a sophisticated software package can cost a lot of time and effort and you will need to assess
the usefulness of any given performance tracking software. When in doubt, keep it simple and accessible
to others. Your tech hub will likely be unique and many lessons will not fit with standard analytical and
documentation software. A master spreadsheet and embedded notes might be all you need and will
make updates and access easier for others.
There is no recipe for how a performance measurement system should look. The process of setting it up is a
valuable learning exercise in itself. You should avoid looking for “cookie cutter” approaches to
performance measurement. You need to understand that you cannot outsource performance
measurement, not even the design of the measurement system itself. In fact, this is often where software
use goes wrong: managers put faith and effort into technology but forget about the basics and their
responsibility. It is better to emphasize the system design process and also involve your staff in it than to
slavishly follow complex software or scorecards. Start with your value creation goals and then define your
indicators (as described in this toolkit), and you will be able to design and set up your own (simple)
system. Easy-to-use and simple desktop/mobile software and online tools like spreadsheets, note-taking,
mind-mapping, and content management systems are probably enough.
Establish a timeline and insert your measurements in intervals that are appropriate for each indicator. A
performance measurement system inherently becomes more valuable over time. If you set up an
accessible and focused system, you will increasingly gain knowledge that will be clearly documented in
the system. Once you have a number of measurements, you will find that you can easily derive learnings
by looking at and analyzing the evolution of important metrics. You will see that many indicators confirm
your intuitions, but others might not, or they might make you aware of nuances, and you will be able to
conduct ongoing self-checks. Naturally, you will need to consistently fill in your findings and results at
specified time points for trends to become obvious. Which interval is most useful will again depend on
potential legal or funder requirements, but also on a given indicator and the availability of data or the
reporting structure that your clients agree to. For example, you might find it useful and viable to track
your client entrepreneurs’ revenue for every quarter, while you might run a satisfaction and feedback
survey only once a year. Bigger evaluations, for which you or an outside agency conduct multiple
interviews and analyses to derive broader data, might only happen once every two years.

    8.2. Use Lean Start-up-style Hypothesis Validation to Extract Better Learnings
Continuous performance measurement and analysis are essential within a “Build, Measure, Learn”
framework. Measurement, analysis, and learning for tech hubs are ongoing and should never be an
afterthought. Large, holistic evaluations in two-year cycles or at the end of a particular programming
period are useful to understand the bigger picture and derive in-depth knowledge. But such post hoc
evaluations do not replace the need for ongoing learning. In the highly complex and uncertain innovation
ecosystems that tech hubs interact with, there are hardly any tested and universally valid implementation
and business models. This implies that a tech hub will likely perform worse than it should if it “flies blind”
until a large-scale evaluation is conducted.
Your measurement should do more than tracking. It must also compare your findings with your projections
and estimates. This kind of hypothesis testing will yield more meaningful insights as the thought process

                                                                                                           34
that you go through when making your estimates will make you aware of the assumptions you make and
beliefs you have about how your services relate to your value proposition.
Ideally, at the beginning of every time period and for every indicator, you can make crude estimates of the
results at the end of the period. You can make one very risky or optimistic hypothesis and one very
conservative one. When you see the results at the end of the time period, you will be able to validate or
disprove the hypotheses and assumptions that you made when planning your services and your impact.
This works both for quantitative and qualitative indicators.
Such data-driven hypothesis testing is very similar to the Lean Start-up methodology that is widely seen as a
success factor for tech start-ups and other early-stage or small and medium-sized businesses. The basic
idea behind the methodology is that business model and product innovations inherently advance into
uncharted territory where evidence has to be collected to iteratively improve decision-making. At the
same time, the method is particularly suitable for small start-ups with limited resources, as it emphasizes
the need to keep business modeling and learning-oriented measurements agile and lean. In other words,
in order to gain insights on its business, a venture does not necessarily need a large analytics department
or build complicated statistical projections, but should instead rely on deliberate small real-world
experiments that help validate or falsify hypotheses, which then leads to adjustments. Of course, this
implies that a manager also has to be honest and admit when an assumption turns out to be false—these
can actually be the most valuable lessons, leading to pivoting away from the wrong track.
While the method works particularly well for software and web-based start-ups where extensive data
collection from users is easier, it can also be applied to tech hubs. Tech hubs also work in uncertain and
complex environments where not much data is available and many implementation options and service
portfolios are possible. Iterative, lean, and simple hypothesis testing, based on a short list of important
indicators, is likely to be more helpful in maneuvering innovation ecosystems and improving a tech hub’s
value proposition.
 “Get Out of the Building” to test hypotheses in interactions with your clients. Ultimately, you provide value
to your clients—early-stage, growth-oriented mobile app entrepreneurs. This means that they are the
main source of information that can help you understand what you are doing well, what you could do
better, or what you should not do at all. Setting up a performance measurement system thus does not
replace direct contact with your clients; instead the two are complementary: the information you get
from your clients will feed into the indicators in your system, and the system, by providing you with
documentation and an overview of all measures over time, will give you ideas for new hypothesis tests
that you can bring into conversations with your clients. In other words, you need to continue to “get out
of the building” to engage with your clients and understand and sense their demands and concerns. This
is another key pillar of the Lean Start-up methodology (usually referred to as customer development).

    8.3. Budgeting, Planning, and Staffing for Continuous Analytics
You will have to plan for resources and time for business analytics and maintaining your performance
measurement system; usually a monthly in-depth analytics session is helpful. As you have seen from this
toolkit, business analytics is an ongoing process and not a one-off, ad hoc initiative. This implies that you
need to plan for resources and not underestimate the effort that this will take. You will develop a routine
and rhythm when to delve deeper into the analytics. A monthly in-depth session that can, for example,
precede strategy meetings with your advisory board or consortium is a good rule of thumb. In the hectic
day-to-day schedule of an mLab or mHub manager, it will be tempting to skip these regular sessions.


                                                                                                           35
However, mLab managers have reported that regularity is vital to consistently observing changes and
progress and not getting lost in “putting out fires” and running from one urgent but overall minor
implementation issue to the next. Your performance measurement system should also be set up to blend
in with your daily work. You should frequently look up data and document notes and findings.
As a rule of thumb, you as the manager should work with one staff member who spends a significant
amount of time on measurement, evaluation, and analytics tasks. It is usually a good idea to dedicate
business analytics tasks to one person on your team to guarantee consistency and coherence. Business
analytics functions usually go hand in hand with knowledge management, strategic advisory, and multi-
stakeholder feedback management, so you might think about charting out the terms of references for
this person so there are synergies with such tasks.

    8.4. Involve Outside Help
Research and consultancy organizations can
help you with in-depth evaluations, in          Box 13: mLab East Africa Learning and Improving from
particular, in the context of inflection points Evaluation
for your business model and strategy. Once      mLab East Africa was the first mLab to get off the ground
you developed a performance                     and also the first to conduct a comprehensive evaluation
measurement system that works for you,          conducted by the University of Nairobi. While some
you should be able to stay on top of your       findings were expected, others were surprising and shaped
regular tracking and learning exercises.        the mLab’s future decisions. For instance, incubatees
However, tech hubs have many indirect           clearly demanded more personalized mentorship and Pivot
effects on the innovation ecosystem that        East participants called for better follow-on support for
are difficult to find and assess without a      finalists, which later became key factors for the design of
team of research and evaluation experts. In     the mLab’s virtual incubation program.
fact, the unexpected and initially
                                                Sources: http://www.slideshare.net/tonnyomwansa/pivot-east-2013-
unmeasured effects of tech hubs can be          university-of-nairobi-research;
just as important as, or even more              http://www.ihub.co.ke/blog/2014/02/pivot-east-2014-calling-for-
important than, the effects that are            applications/
commonly anticipated and tracked (see
box 13). For such broader evaluations that aim to get at the systemic and far-flung impact that your tech
hub has, it is recommended that you engage an outside agency that has the required expertise.
Measurement of more complicated (for example, systemic) value creation and impacts is usually not
something that you can do in-house. You should set the incentives in a way that the agency will give you a
neutral and honest but constructive assessment.
It might help you to develop informal feedback channels to critical, informed outsiders, as they will help you
to see new pathways and missing pieces in your business model. Just like start-ups benefit from having
mentors, a tech hub manager is can also benefit from getting constructive but critical feedback from
someone who is not on the team. It is natural that a manager or leader of an organization is not well-
positioned to see certain issues in the organization, as it is easy to lose the distance that is necessary to
reflect on the “bigger picture,” that is, the broad strokes of strategic direction in the context of a given
market and ecosystem. It would be advisable to identify someone on the fringes of the ecosystem as this
will make it more likely that the feedback is relevant but still includes fresh ideas that are not already
circulating in the ecosystem. Such inputs will help you to avoid complacency and biases.


                                                                                                             36
9. How do mLabs and mHubs Work with infoDev?
If you have a performance measurement system in place, you stand to benefit from informally keeping
infoDev in the loop and accessing its expertise and feedback. While infoDev cannot and should not
micromanage your mLab or mHub, it will be better able to support you if it is aware of your current
progress and problems. Sharing reports and overviews pulled from your performance measurement
system with infoDev, or even giving an infoDev colleague access to the system, is an easy way to facilitate
a continuous knowledge exchange. The more you share, the easier it will be for your infoDev counterpart
to give you advice and understand your local context. infoDev will use the insights that you share to give
you feedback, introduce you to contacts from across its network, and give you additional support suited
to your situation.
You should plan far ahead of deadlines for the agreed formal reporting rounds with infoDev. Similar to
informal regular exchanges, the regular reporting that you will be expected to conduct should be
seamless if your performance measurement system is set up in line with what you agreed with infoDev
early on. Formal reporting will include both results reporting (implementation quality/output indicators
and value proposition/outcome indicators) as well as other grant reporting requirements such as financial
and fiduciary reporting. It takes time for infoDev as your interface to coordinate the World Bank-internal
grant reporting process, and so it is good advice to start working towards your reporting deadlines early
to accommodate potential follow-up requests and clarifications. infoDev will engage and collaborate with
you in this process, and will help you to proactively seek guidance and clear out any uncertainties about
the reporting. In other words, you can see the performance measurement system also as an investment
to minimize the extra effort for your reporting duties and make it as seamless and smooth as possible.
infoDev will allocate additional resources and time to support the effort whenever performance
measurement and evaluation is geared towards infoDev’s broader goals rather than the immediate goals
specified in your business model. When a given indicator is immediately relevant for you to assess the
value you provide for your clients (for instance, if you suggested the indicator), infoDev will expect you to
budget for business analytics and take care of performance measurement and the evaluation process.
This is based on infoDev’s belief that in the long-run you need to take full responsibility for your mLab or
mHub to make it sustainable. However, there will be instances where infoDev will have a broader
evaluation interest, beyond your business model. For instance, infoDev might be interested in indirect
effects of your activities on certain components of the local innovation ecosystem, or infoDev might have
a separate reporting duty towards its donors relating to inclusive and social innovation goals. If this effect
type is not instrumental to your business model and therefore not immediately relevant for you, it is
understood that infoDev will support you with the data collection and evaluation. For instance, in these
cases, infoDev could send its own evaluators, hire outside help, or make additional budget resources
available to you.




                                                                                                            37
RESOURCES:

Business Model Toolkit for mLabs and mHubs
http://www.infodev.org/highlights/infodev-publishes-business-plan-toolkit-mlabs-accelerate-sme-
competitiveness-mobile

Resources Including Assessments of Tech hubs
The Business Models of mLabs and mHubs – An Evaluation of infoDev’s Mobile Innovation Support Pilots:
http://www.infodev.org/mobilebusinessmodels
Do mLabs Make a Difference? A Holistic Outcome Assessment of infoDev’s Mobile Entrepreneurship
Enablers: http://www.infodev.org/mobile/mLaboutcomes
Infographic - Do mLabs Make a Difference: http://www.infodev.org/infodev-
files/do_mlabs_make_a_difference_infographic.pdf
ANDE Bridging the Pioneer Gap:
http://www.aspeninstitute.org/sites/default/files/content/docs/ande/Bridging%20the%20Pioneer%20Ga
p%20The%20Role%20of%20Accelerators%20in%20Launching%20High%20Impact%20Enterprises%20.pdf
iHub Research report: http://research.ihub.co.ke/uploads/2013/may/1367840837__923.pdf
GSMA report on Kenya: http://www.gsmaentrepreneurshipkenya.com/

Resources Including Market and Mobile Innovation Ecosystem Assessments
Developing Mobile Applications Sector in Afghanistan. A feasibility assessment by infodev:
http://www.infodev.org/infodev-files/afghanistan-mobile-feasibility-assessment.pdf
Deloitte East Africa PE survey: http://www.deloitte.com/assets/Dcom-
Kenya/Local%20Assets/Documents/Private%20Equity%20Confidence%20Survey%202014.pdf
World Economic Forum. 2013. Entrepreneurial Ecosystems Around the Globe and Company Growth
Dynamics. http://www3.weforum.org/docs/WEF_EntrepreneurialEcosystems_Report_2013.pdf
Ghana Entrepreneurship Ecosystem Analysis by KolCo:
http://fletcher.tufts.edu/~/media/D6F84A0F3F474166BBCD3CDFC6235B9E.pdf
Startup Compass, by Startup Genome: https://www.compass.co/

Other Toolkits
ANDE’s Ecosystem Assessment Toolkit:
http://www.aspeninstitute.org/sites/default/files/content/docs/pubs/FINAL%20Ecosystem%20Toolkit%2
0Draft_print%20version.pdf
DIBD BPO Learning Toolkit: http://di.dk/SiteCollectionDocuments/DIBD/BOP-
Learning%20Lab/TOOLBOX.pdf

APPENDICES: Examples of Indicators and Definitions by mLab/mHub Approach

        Appendix 1: Implementation Quality Indicators

        Appendix 2: Value Proposition/Outcome Indicators


                                                                                                   38
INDICATOR                                    DEFINITION
Number of businesses applying for a          These are the mLab businesses/entrepreneurs who
grant                                        applied for a proof of concept and/or any other grant,
                                             participated in competitions with monetary prizes
                                             (hackathons, app challenges, etc..). These could be
                                             internal (provided by the mLab) and/or external grants.
Number of businesses receiving grants        These are the mLab businesses/entrepreneurs who
                                             received grants and/or monetary prizes. Results should be
                                             only captured when a grant is fully disbursed to and
                                             received by a business/entrepreneur.
Amount of grants received by businesses      This is the amount of grants and/or monetary prizes
(USD)                                        received by mLab businesses/entrepreneurs. Results
                                             should be only captured when a grant is fully received by a
                                             business/entrepreneur.
Number of businesses who registered          Number of mLab businesses/entrepreneurs who
their invention with Intellectual Property   registered their product/service/application prototype
(IP) office                                  with Intellectual Property (IP) office.
Number of businesses who release their       Number of mLab businesses/entrepreneurs who use
invention to the community                   appropriate statements or licenses to release their
                                             product/service/application prototype as free software,
                                             permissive open source, or contribute it to the public
                                             domain. Further commercial and proprietary use should
                                             be allowed.
Number of businesses who raised early        Number of mLab businesses/entrepreneurs who managed
stage finance                                to raise product development/testing seed/early stage
                                             finance through venture capitals/crowd funding/angel
                                             investors/other sources. These are
                                             businesses/entrepreneurs who received non-
                                             financial/financial services and/or grants from the mLab.
Amount of early stage finance raised by      The amount of product development/testing seed/early
businesses (USD)                             stage finance raised by mLab businesses/entrepreneurs.
Number of businesses who raised              Number of mLab businesses/entrepreneurs who managed
growth stage finance                         to raise growth stage (commercialization) finance through
                                             venture capitals/crowd funding/angel investors/other
                                             financial institutions. These are businesses who received
                                             non-financial/financial services and/or grants from the
                                             mLab or other sources.
Amount of growth stage finance raised        The amount of growth stage (commercialization) finance
by businesses (USD)                          raised by the mLab businesses/entrepreneurs.
Number of new direct jobs created            Number of additional new direct fulltime jobs created by
                                             mLab businesses/entrepreneurs as a result of growth.
                                             These includes also fulltime direct additional jobs created
                                             by the business in other countries as a result of business
                                             expansion.




                                                                                                       39
INDICATOR                               DEFINITION
Additional sales revenue for targeted   Additional sales revenue (USD) for the mLab
businesses (USD)                        businesses/entrepreneurs. For established businesses,
                                        result should be captured for the additional sales revenue
                                        above normal growth revenue achieved by the firm prior
                                        to joining the mLab.

Number of digital products/services     Number of digital products/services developed and/or
developed/improved                      improved by mLab businesses who received services from
                                        the mLab and/or any of their non-financial/financial
                                        service providers. Only count digital products that were
                                        developed for commercialization.
Number of digital product/service       Number of digital product/service prototypes developed
prototypes developed/improved           and/or improved by mLab businesses/entrepreneurs who
                                        received services from the mLab and/or any of their non-
                                        financial/financial service providers. Prototypes could
                                        have been developed/improved during a hackathon,
                                        training program, or another activity where with emphasis
                                        on skills and capacity development.
Number of digital products/services     Number of digital applications/services
developed/improved available in the     developed/improved by mLab businesses/entrepreneurs
market with a social and/or             who received services from the mLab and/or any of their
developmental impact                    non-financial/financial service providers that are available
                                        in the market for end users and do have a
                                        social/economical developmental impact. These are digital
                                        products/services linked to public services, health,
                                        education, agriculture, etc
Number of users reached by              Number of users reached by additional digital
developed/improved digital              products/services developed/improved by mLab
products/services                       businesses/entrepreneurs who received services from the
                                        mLab and/or any of their non-financial/financial service
                                        providers
Number of users reached by              Number of users reached by additional digital
developed/improved digital              products/services with social/economical developmental
products/services with social and/or    impact developed/improved by mLab
developmental impact                    businesses/entrepreneurs who received services from the
                                        mLab and/or any of their non-financial/financial service
                                        providers




                                                                                                       40
INDICATOR                                    DEFINITION
\Number of partnerships with non-            Partnerships between the mLab and non-financial service
financial services providers                 providers (mentors/coaches/trainers) providing
                                             technical/legal/business management and operations
                                             advise to entrepreneurs/businesses. These could be
                                             partnerships with firms/individuals on a
                                             contract/voluntary basis
Number of partnerships with financial        Partnerships between the mLab and financial service
services providers                           providers (banks/venture capitals/angel networks/crowd
                                             funders) providing financial advise and/or seed/early
                                             stage/growth stage finance to entrepreneurs/businesses.
                                             These could be formal/semi-formal/informal partnerships
Number of mLab service sessions,             These are any face to face and/or online
including coaching and mentoring,            workshops/training
bootcamps, etc..                             events/seminars/conferences/coaching/mentoring
                                             sessions for mLab businesses/entrepreneurs,
                                             organized/facilitated/sponsored/partnered by the mLab
                                             and/or their financial/non-financial service providers

Number of businesses/entrepreneurs           These are the mLab businesses/entrepreneurs
receiving services                           participating in face to face and/or online
                                             workshops/training
                                             events/seminars/conferences/coaching/mentoring
                                             sessions linked to the mLab and their partner financial and
                                             non-financial service providers only+C23
Number of participants providing             These are the mLab businesses/entrepreneurs providing
feedback on satisfaction                     feedback on face to face and/or online
                                             workshops/training
                                             events/seminars/conferences/coaching/mentoring
                                             sessions linked to the mLab and their financial and non-
                                             financial service providing partners. The feedback is
                                             mainly on the venue, service provider knowledge,
                                             materials used and relevancy to their need.
Number of businesses/entrepreneurs           These are the mLab businesses/entrepreneurs reporting
reporting satisfied or very satisfied with   satisfied and/or very satisfied feedback on face to face
mLab services.                               and/or online workshops/training
                                             events/seminars/conferences/coaching/mentoring
                                             sessions linked to the mLab and their financial and non-
                                             financial service providing partners. The feedback is
                                             mainly on the venue, service provider knowledge,
                                             materials used and relevancy to their need.
Number of media appearances                  Number of local/regional/international media
                                             appearances in TV/radio/press, for the mLab and/or their
                                             businesses/entrepreneurs




                                                                                                           41
INDICATOR                                   DEFINITION
Number of knowledge sharing events          Number of local/regional/international knowledge
                                            sharing/networking events/activities that mLab
                                            organized/facilitated. At the mLab level these could
                                            include operational learning, best practice, or the the
                                            journey of a successful business/entrepreneur.
Number of knowledge products                Number of informative publications and/or visuals
developed                                   produced by the mLab on success stories and lessons
                                            learned. The knowledge pieces cover both
                                            business/entrepreneurs and internal mLab
                                            management/operational topics.
Enabler cost to sustainable revenue ratio   This is a ratio of mLab cost (both fixed and variable) to
                                            sustainable revenue. Sustainable revenue may include
                                            different sources (government, fees paid by
                                            businesses/entreprenuers, royalties, even donor funding),
                                            depending on the mLab's business model. Seed funding
                                            (such as infoDev grants) should not be considered as a
                                            revenue. A ratio of 1 means that the mLab is at breakeven,
                                            less than 1 means that more revenue is generated than it
                                            costs to run the mLab, and more than 1 means that costs
                                            incured are more than revenues generated.




                                                                                                     42
© 2015 infoDev/The World Bank | 1818 H Street NW | Washington DC 20433
Email: info@infoDev.org | Telephone: +1 202-458-8831 | Twitter: @infoDev
                                                                   39
www.infoDev.org
