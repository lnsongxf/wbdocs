TRANSPORT AND ICT




Open Data Readiness
Assessments: Lessons Learned




                        JUNE 2017
                                                                                                      i




Acknowledgments
This report was prepared by Andrew Stott, Senior Open Data Consultant, World Bank. Thanks to Tim
Herzog, Pierre Chrzanowski and to colleagues within the World Bank and external experts who have
contributed to and commented on this paper in draft. We would like recognize the financial contribution
of Trust Fund for Statistical Capacity Building which made this research possible.



Standard Disclaimer
This note is a product of the staff and consultants of the International Bank of Reconstruction and
Development/The World Bank. The findings, interpretations, and conclusions expressed in this paper do
not necessarily reflect the views of the Executive Directors of the World Bank or the governments they
represent. The World Bank does not guarantee the accuracy of the data included in this work.



Copyright Statement
This note created by The World Bank, is available under the Creative Commons Attribution 3.0 Unported
(CCBY3.0) license.
Open Data Readiness Assessments: Lessons Learned
                                                                                                                                                               1




Table of Contents
                       ........................................................................................................................................ i
            .................................................................................................................................................. 2
                                                               ............................................................................................... 3
                                                           ................................................................................................... 3
                 ............................................................................................................................................. 3
                           ................................................................................................................................... 6
                                                    .......................................................................................................... 7
                        ...................................................................................................................................... 9
                          .................................................................................................................................. 10
          ................................................................................................................................................... 11
                                  .......................................................................................................................... 11
                                                                  .......................................................................................... 12
                                                                            ................................................................................ 12
                                                ............................................................................................................ 12
                                                                            ................................................................................ 13
                                     ....................................................................................................................... 14
                             ............................................................................................................................... 14
                           ................................................................................................................................. 14
                                                          .................................................................................................. 15
                                    ........................................................................................................................ 15
                                 ........................................................................................................................... 16
                                             ............................................................................................................... 16
                                                                ............................................................................................ 17
                                                                        .................................................................................... 17
                                                                ............................................................................................ 17
Open Data Readiness Assessments: Lessons Learned
                                                                                               2




Introduction
This paper is intended to capture the main lessons learned from conducting Open Data Readiness
Assessments and assisting countries with their implementation. It looks at three main areas:

          Common issues arising in the content of the ODRA reports of different countries,
           arranged by the eight main dimensions of the ODRA methodology. Some of these
           issues have a wider impact in addition to their relevance to open data.

          Common issues arising in the implementation of ODRA reports in different countries.
           These issues are relevant to further open data projects and may suggest the
           development of further implementation tools in the World Bank Open Data Toolkit.

          Lessons learned in applying the methodology used for ODRA – now on its third version.
           These lessons from practitioners are relevant to any further open data readiness
           assessment and to the use of similar methodologies in other areas of the Bank’s
           practice.

Where appropriate examples have been cited, sometimes by name of the country involved where
the lesson learned was positive. Findings one way or the other should be not be inferred about
countries not mentioned.

This paper deliberately does not make specific recommendations for changes in the ODRA tool
or open data programs, but it is hoped that its findings would be taken into account if, at some
point in the future, the ODRA tool and implementation guidance were to be refreshed. In addition,
by capturing observations from experienced ODRA practitioners, It could also be useful
background reading for those conducting ODRA reviews or open data work in developing
countries for the first time.
Open Data Readiness Assessments: Lessons Learned
                                                                                                  3




1. Common Issues Arising in ODRA reports
1.1 This section follows the structure of eight dimensions of the Open Data Readiness Assessment
methodology.

Political Commitment and Leadership
1.2 In many ODRAs it has been possible to find general statements supporting transparency from
top political leadership, but it is rare to find these specific to data. Importantly although such
statements often exist, they are often not communicated within the public service and relevant
officials do not use them to develop detailed policies.

1.3 It is more unusual to find the same sort of statements in relation to the digital economy or
the economic use of data, although in some countries (such as Botswana) economic
diversification is a recognized top political goal.

1.4 Some of the countries where ODRAs have been conducted are members of the Open
Government Partnership, although often the OGP membership has been the result of diplomatic
pressure rather than by building on local open data initiatives and advocacy. Many action plans
include intentions on open data – either in general terms or on opening specific data (such as
contract data, budget data or extractive industries data). In some cases, the ODRA itself is the
action in the OGP action plan. However, in some countries there has seemed to be a disconnect
between the action plan communicated internationally and action within the government itself,
particularly where responsibilities are not in the same organizational unit: plans have been
committed to the OGP Secretariat without matching implementation plans within government –
or sometimes without communicating the international commitment within government at all.
In some cases, OGP actions on open data or transparency did not seem to have been clearly
assigned or resourced within government.

1.5 In countries that were not members of the Open Government Partnership there seemed to
be little awareness of the OGP and its aims.

Policy issues
1.6 Many developing countries taking part in ODRAs have not yet enacted and implemented the
full range of relevant legislation and policies for Open Data including Freedom of
Information/Access to Information Acts and Data Protection Acts. Many have stated intentions
to do so, but the necessary legislation has not been considered by the legislature or, even if it is
on the statute book, it has not been implementing in practice with appointed officers in each
Ministries, training for relevant staff, and the appointment of independent Commissioners (in
the countries where the law requires this). For Commonwealth countries, there are model Bills
and associated technical assistance to help countries start on these legislative instruments, and
Open Data Readiness Assessments: Lessons Learned
                                                                                                     4


there is a wider body of best practice on which to draw. Indeed, in several countries the legislative
drafting section of the Attorney General’s Chambers had drafted the legislation but there was no
political priority to taking it through the legislature.

1.7 Many Commonwealth countries have inherited an “Official Secrets Act” from their British
past. The original British Official Secrets Act was enacted during a spy scare shortly before the
First World War. Although this legislation has been significantly amended in the UK itself, other
countries still have it in its original draconian form. This not only makes it difficulty or risky for
individual officials to authorize the release of data but also engenders a culture of official secrecy.

1.8 More generally in many countries there was a lack of clarity about who could authorize the
release of data. This often applied to requests for data under existing arrangements – officials
felt the need to get approval from the top of the Ministry to release any data, because they felt
that they would be criticized if they took the wrong decision.

1.9 Charging policies have often been found to be inconsistent and to have unclear objectives.
Common findings have included:

          Although many Ministries find it difficult to explain the rationale for charging for
           information or the basis on which prices have been set. In many cases the charges
           have not been regularly reviewed.

          Charges for information previously provided on paper are often being applied to
           providing information in digital form – even though the costs of distributing the
           information are very different.

          The requirement to collect charges, and the difficulty of collecting charges
           electronically, sometimes mean that data users need to visit the Ministry personally
           in order to pay the charge and collect a machine-readable version of the data.

          In most cases, any charges collected are remitted direct to the Ministry of Finance and
           do not actually go to cover the actual costs of the data-owning Ministry in distributing
           the information and the cost of collecting the charges. As a result, the greater the
           demand for paid-for information the greater the administrative burden on the
           Ministry.

          In most cases the charges collected each year for the sale of information are less than
           1% of the total budget of the Ministry concerned. This has been found frequently to
           be the case even in functions, such as mapping and meteorology, which in developed
           countries have sometimes historically been largely on a cost recovery basis.
Open Data Readiness Assessments: Lessons Learned
                                                                                                  5


          In some cases, it has seemed possible that the charges do not even cover the costs of
           collecting the charge, even when viewed in terms of government finances as a whole.
           In such cases the charge does not make fiscal sense, although in at least one case the
           Ministry of Finance considered that the charge nevertheless served a “gatekeeper”
           function.

          Some Ministries of Finance are expecting their MDAs to do more cost recovery and
           charges for supply of information are being considered. This was the direction of
           policy in Europe in the 1980s and 1990s, and has subsequently been seen as sub-
           optimal in terms of overall economic and social value; European countries are now
           moving to eliminate charges for information supplied electronically.

1.10 Most countries and individual Ministries have not given much consideration to policies of
copyright and licensing of government information.

          On the web, websites often have a simple “copyright notice”. However, this is rarely
           in the form required by the Berne Convention and has often been inserted by the
           website developer rather than as considered policy by the Ministry itself. Websites
           often say “All Rights Reserved” – even where information is provided for download.

          For information supplied on request, many Ministries, Departments and Agencies
           require those wanting information to say how it will be used so there may be an
           implicit permission to use it for the purposes originally proposed.

          Even in the implementation phase some countries find it difficult to select and
           implement a proper open license (such as Burkina Faso, Sierra Leone, Haiti and
           Uganda) and even where a recognized open license has been used (such as CC0 for
           the Kenya Open Data Initiative) it is not always clear that this has been the result of a
           considered policy and legal position.

1.11 For some Ministries, Departments and Agencies a sensitive issue in licensing has been the
ability of a third party to re-distribute the data. Sometimes this has been because the government
want authoritative data to be seen as coming from them or because the Ministry want to be able
to control who else can access the data and for what purpose. In some cases, while Ministries
have been prepared to allow free redistribution of data, they have reacted strongly to the
possibility that third parties could charge for re-distributing the data; this has seemed to stem
from the risk that a private company would charge citizens for data that they could obtain from
the government for free.

1.12 More generally Ministries, Departments and Agencies have been concerned about the
commercial use of data supplied by a government free of charge. The concern has usually seemed
Open Data Readiness Assessments: Lessons Learned
                                                                                                6


to be one of equity, that the data provider should share in the profits generated from the use of
the data. The economic reasoning – that total national welfare is maximized if government data
is distributed at the marginal cost of distribution – is perceived as a less certain mechanism for
sharing the benefits.

1.13 One of the reasons for the under-development of policy is that, before Open Data and the
“data revolution” more generally, information may not have been regarded as a valuable or vital
resource by the Government and so information-related issues do not have an established “home”
within the structure of government. For instance, in one country the Attorney General’s
Legislative Drafting Unit had done work on preparing an Access to Information Bill but there was
no clear “client” Ministry developing the policy for the Bill or planning its implementation.
Similarly record and archive management has had a low priority.

1.14 The International Open Data Charter, agreed in 2015, has proved to be a useful checklist for
the development of open data policies, for instance in Tanzania, Saint Lucia and Jamaica. In
implementation support since 2015 it has been used to assess draft open data policies and
suggest areas where the client government should further develop their proposals. The prospect
of compliance with an international standard has proved a valuable incentive towards the
adoption of good open data policies.

1.15 Development partners and private companies implementing and managing data projects in
the countries can also prevent the spread of open data principles into Governments. In the worst
cases this has involved claiming data ownership (budget data in Cameroon), and implementing
or advocating for business models not in line with open data principles (map in Burkina Faso). In
other cases, partners have been unwilling to engage on this issue because they fear the
Government’s reaction, and therefore for their commercial interest. More generally there has
also been a lack of awareness of open data among these stakeholders.

Institutional issues
1.16 In the countries that have been most successful in open data the initiative has had not only
political leadership from the highest level but also institutional leadership from the center of
government. For instance, in the Ulyanovsk Region of Russia the ODRA counterpart was an
adviser to the Governor, and he was in a position to ensure effective follow-up action that led to
Ulyanovsk being seen as the leading Region on open data. In Indonesia, the open data initiative
was led by the President’s “Implementation Unit” and its influential Minister, with an extensive
existing record of success in putting cross-government policies into practice. In both these
countries the ICT function of the government did not play a leadership role.

1.17 However for most ODRAs the Ministry of ICT or a similar body has often been seen by
governments as the most appropriate agency to lead on open data, and in most cases, they have
been designated as the counterpart team to support the ODRA. In part this is because in
developing countries there appears to be more centralization of ICT responsibilities – either in a
central Ministry or in Government company (or both) than is typically the case in countries at a
Open Data Readiness Assessments: Lessons Learned
                                                                                                  7


later stage of development, and therefore they seem a logical point to co-ordinate what is seen
as a technical initiative.

1.18 In a few countries the ICT Ministry has made progress when there is a clear political
sponsorship – either from the Office of the President or equivalent or from an individual
influential Minister. However, there is the risk that Ministerial changes can lead to a loss of this
sponsorship -as happened in Antigua and Jamaica.

1.19 So while ICT Ministries have a key role to play in the technical implementation of open data
initiatives – and in improving data management more generally – many do not have the
necessary skills or experience in cross-government non-technical policy formation, or do not
alone have the authority to ensure implementation in other Ministries.

1.20 It had been expected that National Statistics Offices would often take leadership and
ownership of open data initiatives – both because their own data was some of the most valuable
and because in the course of their work they would already have developed capacity in important
skills such as data management, metadata and anonymization. So, for instance in Tanzania the
National Bureau of Statistics is indeed one of the leading agencies in the Open Data Taskforce.
However, while most National Statistics Offices have been supportive and in some cases (such as
Uganda, Jamaica, Saint Lucia and Mauritius) very keen to improve the publication of their own
data, there has been less enthusiasm than expected among NSOs to take overall leadership of
open data initiatives themselves. (This is an issue in some higher income countries as well.) The
World Bank has convened a separate working group of NSOs to specifically look at the issue of
roles and leadership in open data, and has published two white papers on that subject, which are
in the open data toolkit1.

1.21 Many Ministries of Finance have been supportive at senior levels of greater budget
transparency and the transparency/open data agenda more widely. This was seen in countries
such as Saint Lucia, Jamaica and Trinidad where the top officials in the Ministry of Finance
engaged directly in the ODRA.

Data holdings and release of data
1.22 Before the ODRA program started there was an expectation that most of the data in
developing countries would not exist in digitized form. Yet in fact a surprising amount of current
government information in the countries assessed has been found to be digitized: although
information may be collected in paper form, it is often entered into spreadsheets or simple
databases for analysis at the level of national Ministries. Many countries have projects to extend
data gathering to the point of creation, including the use of SMS where internet connection is




1

                     and
Open Data Readiness Assessments: Lessons Learned
                                                                                                            8


not possible2. Digitization of potential valuable data is also happening as part of administrative
modernization – for instance moving to computer-based business registration systems.

1.23 However in many countries senior levels of Ministries do not have a full appreciation of the
data that is already collected and held by government. In one country, the top official in charge
of educational reform bemoaned the lack of disaggregated school-by-school data, not knowing
that the data was actually collected and digitized at that level by his own Ministry before being
aggregated into national education statistics. Conversely in some other countries individual
leaders were driving the collection of better data: for instance, in Trinidad and Tobago the
Minister of Education had initiated a new data collection system to give him detailed and up to
date data about individual schools.

1.24 A related finding was that in some countries senior officials may not have a full appreciation
of the data that is already published. In one country, the head of the Education Ministry was
reluctant, on confidentiality grounds, to publish open data down to the level of individual schools,
not realizing that the data was already published annually in an Education Digest.

1.25 Many historical records have not been digitized and many Ministries, Departments and
Agencies do not have an intention to do so. So, past statistical reports may only be available in
paper form. Where some digitization has happened, it has tended only to be the extent needed
for internal operational purposes – for instance often only the last 20-30 years of past weather
observations have been digitized. While a few applications (for instance insurance against
extreme natural events) would ideally require a longer historical record, most of the value in
open data lies with current or recent data; ODRAs have not normally found the digitization of
very old records to be a priority.

1.26 That having been said, in some cases there is very old data still in use – for instance
topographic maps where there is typically relatively little change in natural features over decades.
Some countries are making progress in digitizing and updating their maps, some as part of wider
National Spatial Data Infrastructure initiatives. But in others only paper maps based on old
surveys, or raster scans of those paper maps, are currently available. Given that in the EU about
30-50% of the total value of Open Data was in geospatial data, and that geospatial data is one of
the key datasets most frequently requested during ODRAs, recommendations have sometimes
been made not only to release the data but to invest in collecting and digitizing it where it does
not currently exist in digital form. (Such an investment is also likely to have value for the public
sector.)

1.27 Where existing data is published it is often in the form of PDF documents – even where the
original data is held in machine-readable form such as a spreadsheet or a database. There appear
to be a number of reasons:




2
  For instance the Society for the Elimination of Rural Poverty in Hyderabad, India, have implemented a multi-
technology data capture system for their field workers, including SMS.
Open Data Readiness Assessments: Lessons Learned
                                                                                                             9


          The PDF file may be produced during the production process leading to a paper report;
           putting the same PDF file online is a simple process and does not require additional work
           by the content owner.

          Publication of the data in paper or PDF form is seen as meeting the need for transparency.
           There is often little appreciation of how people would use large amounts of information
           for transparency purposes – for instance sorting and searching a 1100-page annual
           “budget book” – and so no appreciation of how much more useful re-usable data would
           be. (A cynic might say that publication in PDF form was designed to frustrate meaningful
           accountability, but there is little evidence that that is the motivation.)

          Some data owners say that the main reason given for using PDFs is to ensure that the data
           is not changed. PDFs are seen as giving transparency without the ability to re-use or
           modify the data. This is a particular concern of those, such as National Statistics Offices
           or Election Commissions, where the accuracy of the information has wider national
           importance. However, none of such PDFs have been found to be digitally signed with a
           verifiable key nor accompanied by verification instructions, so the way PDFs are being
           used does not meet this policy requirement as well as requiring a simple reference back
           to authoritative data on a secure government website would do.

1.28 In the case of Africa agencies at least two countries3 noted the African Development Bank
Open Data for Africa portal4 as demonstration that they were already publishing data. While use
of this portal can demonstrate good intent and can help build capacity in extraction and
transformation of data, it is not a full substitute for a national open data program or a national
open data portal – for instance the AfDB platform seems designed for statistical indicators rather
than all data types and some of the data appears to have come from international organizations
or external sources rather than actually coming from country MDAs themselves.

Demand for data
1.29 Most countries assessed had both activists and developers from civil society that were very
interested in obtaining government data and in using it, at least in simple ways. Where data was
available in re-usable form there was often evidence of it being turned into simple applications
(such as agricultural market price information systems). Where data was not available some local
developers had been innovative in obtaining non-government data by other techniques such as
crowd-sourcing. However, there was little evidence from ODRAs that civil society were prepared
to expend the amount of effort required5 to make large PDFs analyzable (so, for instance, ‘budget


3
    Such as the National Treasury of Kenya
4
5
 La Nacion in Buenos Aires and Fair Play Alliance in Slovakia have used crowd-sourcing techniques to turn large
collections of PDFs into processable data. This takes considerable organizational skills.
Open Data Readiness Assessments: Lessons Learned
                                                                                                10


books’ published as PDFs were not being recoded to be used in “Where Does My Money Go”
models).

1.30 In many countries assessed by ODRAs academics were a more important user and
stakeholder in open data than in more developed countries. This is largely because, in the
countries involved, detailed statistical information and supporting administrative data was not
readily available to researchers. In some countries, some of it was available on request and if the
proposed research was approved, but this was seen as a time-consuming and cumbersome
process. Academics were also involved in both civil society and business uses of data – for
instance the University of the West Indies were active in developing agricultural applications in
both Jamaica and Trinidad & Tobago.

1.31 Journalists were in principle interested in open data, but in many countries they reported
that staffing and economic pressures militated against the development of analytical and
investigative stories. However, there were sometimes individual independent journalists who
had some data and programming skills and were pursuing data-driven stories.

1.32 In most countries businesses reported a lack of the basic data they needed to optimize their
businesses and to develop new market opportunities. Often the information required was
detailed statistical information on trade, labor market and demographics. There were also
indications in some countries that the lack of certain data was inhibiting the development of
business-to-business services needed to support growing business models: for instance, a lack of
geospatial data could inhibit the development of a cost-effective retail logistics sector, that in
turn inhibited the development of e-commerce.

Civic Engagement
1.33 In most cases there was little evidence that Ministries, Departments and Agencies engaged
with the actual or potential users of their data – even the data that was already published. In one
case the National Statistics Office told the ODRA that no users had been consulted in the
preparation of its new strategy.

1.34 Most Ministries, Departments and Agencies did not have data on how much existing data
was used. Although they believed that information on the number of downloads from their
website could be produced, they did have the information at hand and it usually proved
impossible to produce it within the timescale of the ODRA. Similarly, most Ministries,
Departments and Agencies did not know how many requests for information they received or
how many of these were met.

1.35 Nevertheless many Ministries did seem to sincerely believe that they were open to requests
for data – which was very different from the perception of their stakeholders.

1.36 Many Ministries had websites. However, they were often out-of-date (for instance not
reflecting organisation of government changes) and incomplete. In some cases, the websites
were not operational for prolonged periods or were reported by Google as having been hacked.
Open Data Readiness Assessments: Lessons Learned
                                                                                                  11


The best websites were often those where there was a cross-government content management
system and common content structures for each Ministry.

1.37 Only a small number of countries were actively using social media – mainly Facebook – to
communicate with the public. Where this was being done it was largely to ‘push’ announcements
or news, and there was little follow-up engagement or conversations with the audience.

Finance
1.38 An open data initiative seems uncomfortably to fall between being small enough to be
fundable out of existing business as usual allocations and being large enough to justify a separate
project funding stream. Moreover, many of the costs arise in additional responsibilities for staff
in individual Ministries who need to extract and transform data, and authorize it for publication,
as part of their work.

1.39 Therefore in addition to securing funding for tangible and central costs – such as the central
team and the open data portal – there is a need to ensure that there is agreement to devote the
necessary resources in individual Ministries. This can sometimes best be done by amending job
descriptions of related roles (for instance Access to Information officers in Ministries) and let the
financial consequences be resolved in the annual budget round.

1.40 A further consideration is how to ensure that open data funding moves from a project-based
funding onto a sustainable, business as usual, basis. While there is rarely opportunity within the
ODRA itself to explore how this should be done, it should be an action for the Ministry owning
the initiative to solve before project funding expires. This is particularly important where the
project is funded by a donor and so is outside the normal budgetary mechanisms during the set-
up and initial operation phases.

National Infrastructure
1.41 The ODRA methodology asks about the national infrastructure of telecommunications,
including internet users, mobile subscriptions, broadband coverage and smartphone ownership.
However, while these are important to some applications, the lack of advanced
telecommunications infrastructure is sometimes regarded by both government and stakeholders
as a reason why open data should not be a priority at this stage in a country’s development. In
such cases ODRA teams have pointed to use cases where economic and social value has been
obtained despite technological limitations – including the use of SMS-based applications for
farmers in Africa and the use of blackboard in the main square of Monrovia, Liberia, to engage
the community in budgetary figures.
Open Data Readiness Assessments: Lessons Learned
                                                                                                 12



2. Common Issues in ODRA Implementation
Delay in approving and publishing ODRA reports
2.1 In some countries it has proved difficult to obtain final signoffs to publish the ODRA report –
even once a final draft reflecting the comments from the client and from others has been
presented and even where the findings are already in the public domain (for instance by the
client inviting external stakeholders or the press to presentations during the ‘delivery mission’).
It is usually not clear what the obstacle is, and a more explicit agreement about publication and
its timescale could be useful when the ODRA is initially agreed.

Mobilization of implementation
2.2 A near-universal experience is that it can take a number of months for open data
implementation to gain momentum and deliver initial results. This was found even in the United
States (where in 2009 it took around 240 days to gain agreement to a plan that the new President
had requested in 120) and in the United Kingdom (where an initiative announced in June 2009
did not launch its portal until January 2010). In Ulyanovsk, even with the Governor’s support it
similarly took 6-9 months before momentum was established. There seems to be an inevitable
time lag while decisions and political acceptance is established, resources are allocated, teams
are recruited and formed, policies and processes are developed and agreement to them is
secured.

2.3 In many situations the immediate drive has been to create an open data portal. This is
understandable in that a portal is a practical manifestation of an open data initiative. Creating or
procuring a new computer system is also something that an ICT Ministry, if they are in the lead,
know how to do. However, there is a risk that the portal will be technically delivered without
sufficient open data to make it credible and at the expense of other, less tangible, tasks to ensure
its sustainability. For instance, the Kenya open data initiative had a well-received early launch in
2011 but then did not consolidate the policies; it had to be relaunched in 2014 and there is still
work to do to transition to business as usual. More generally there is a risk that the portal will
itself be declared as a “success” and the necessary policy development and institutionalization
of open data as business as usual will never follow.

2.4 The experience of Saint Lucia and Jamaica – where the World Bank provided support to
implementation as part of a DFID-funded project – was that the implementation of an open data
portal and the extraction and transformation of initial datasets were not on the critical path of
the open data projects, and that development and securing agreement to policies and
responsibilities took much longer.

2.5 A related issue is that actions recommended in ODRA reports inevitably tend to be front-
loaded, with many actions that could be taken at an early stage and without dependencies. This
can present the open data team with a daunting agenda for the first six months, at a time when
they are still setting themselves up and when they are still establishing their influence and
Open Data Readiness Assessments: Lessons Learned
                                                                                                  13


authority across government; it means that almost inevitably that the team will fall behind the
schedule in the report. However, conversely, it could be difficult to “sell” a less ambitious plan to
donors or to the government itself.

2.6 One of the reasons for requiring a “counterpart team” was to help develop knowledge and
understanding during the ODRA process among those who would then take on the mobilization
and leadership of the implementation. Where this happened – for instance in Ulyanovsk and in
Saint Lucia – the project leader then had a good understanding of what needed to be done –
beyond the bare words in the ODRA report. By contrast where the prospective leader of the open
data initiative did not personally take part in the ODRA process it was harder to achieve early
momentum.

2.7 It can also be important for the World Bank team that conducted the ODRA to follow up and
continue to push for implementation. Better results have been achieved where the team
established very good close relationships with the counterpart and maintain this momentum
through phone calls, meeting in conference, and planning next steps.

Continued engagement of external stakeholders
2.8 Engagement with external stakeholders is an important part of the ODRA process itself, but
the pause for mobilization can mean that continuity of engagement is lost. The most successful
open data teams work both within government and with the users of their data and other
external stakeholders. External stakeholders, unfettered by the constraints of official practice,
can sometime be powerful allies in lobbying for action; and it is the use of data by users that can
be most convincing of the value of an open data initiative.
Open Data Readiness Assessments: Lessons Learned
                                                                                              14



3. The ODRA Process Itself
3.1 The design goals of the Open Data Readiness Assessment methodology were to give a relative
quick and cheap diagnostic tool that would lead to recommendations for action on open data
and that could be flexibly deployed at national, regional and municipal level by a range of open
data experts. To this end considerable thought was given to how to use the study time to the
best effect and, in addition to the assessment framework itself, a User Guide was written to
capture best practice in applying the methodology. While the basic structure of the assessment
framework has been largely unchanged since the original version, the user guide has been
progressively extended in order to reflect experience from the conduct of the ODRAs. This section
capture further lessons learned that could be useful in future ODRA – and in designing similar
methodological approaches in other fields.

Deployment Models
3.2 The original deployment model for the ODRA was as a World Bank assessment using World
Bank staff and international consultants. However, the World Bank not only sought extensive
external input into the design of the tool but also “open sourced” the methodology and its user
guide for others to use. In addition to the World Bank’s own use the methodology has been
successfully deployed by:

          Other Development Partners, such as the UN Development Program.

          Independent consultants in Asia

          Governments to assess themselves (such as in Kazakhstan)

3.3 Doing so has not only leveraged the World Bank’s original investment but has also given the
World Bank greater thought leadership in the field by its ownership of a methodology that others
have adopted.

Client sponsorship
3.4 The original model of the ODRA was that it should have a very senior sponsor from the client
government. This was seen as a way of ensuring that the assessment team gained the necessary
access and as an indication that there was sufficient top-level will to take open data forward to
make the investment in the assessment worthwhile. In the first round of ODRAs this was strongly
present – for instance through the Deputy Governor of Ulyanovsk and the relevant Minister in
Antigua. However, in some subsequent ODRAs the client sponsorship was weaker or at a more
operational level; although this rarely led to serious shortcomings in access during the
assessment itself it did lead to problem in translating the ODRA report into action as senior
decision-makers did not feel that they had owned the production of the report.
Open Data Readiness Assessments: Lessons Learned
                                                                                                 15


Locating the right people to interview
3.5 It has sometimes proved difficult for members of the counterpart team to identify the right
people to be interviewed during the ODRA evidence gathering. This has been partly because
Open Data raises issues that the government do not normally consider – such as issues of the
ownership and licensing of government information. It has also been because some counterpart
teams only have networks among ICT colleagues and the right level of policy official is not known
to them. It has been found that the best outcomes can come by: asking to see in each Ministry a
policy official at deputy secretary or director level as well as the ICT officer (so not getting one
but not the other); the counterpart team initially identifying people but discussing their role and
position with the ODRA team before confirming the interview; and where roles are unclear trying
to see a senior official early in the interview program so that follow-up interviews can be arranged
as necessary.

3.6 Similar issues can arise on the demand side. Some Governments are poorly placed to find civil
society stakeholders. In these cases, the local WB office or contacts they suggest can be helpful;
in addition, regional civil society and academic networks such as Caribbean Open Institute or
Code for Africa. In Botswana, social media was also successfully used to find interviewees from
civil society. Conversely in Trinidad and Tobago the Government had a comprehensive list of
organizations on its Civil Society Board and the ODRA team and its counterparts made a selection
of those likely to be interested in data, and this led to a very successful engagement.

3.7 In engaging civil society it was found fruitful to seek to engage not only with transparency-
related organizations but also with organizations with thematic campaigns such as the
environment (as was done in Trinidad and Tobago). Thematic campaigners were interested in
data relevant to their interests, not just conventional ‘transparency’ data such as budgets; and
thematic campaigners sometimes had very clear use cases in mind for their data.

3.8 Engagement with businesses had similar issues. The first point of contact was often Chambers
of Commerce and Business Associations, but they themselves had different data requirements
(for instance national aggregates) than individual businesses (who tended to want more
disaggregated data). The most successful model seemed to get chambers of commerce and
business associations to reach out to their members and to organize round tables for the team
to meet.

3.9 For civil society, academics, journalists and businesses the most time-effective means of
gathering evidence was usually found to be separate round-table meetings of 8-10
representatives for each sector. This allowed the meetings to focus on the needs of the sector so
all participants felt engaged, and to allow contributions to build on one another and to establish
some sense of priority and readiness for particular types of data and use case.

Timetable for interviews
3.10 In the ODRA guidance the counterpart team is expected to arrange the timetable for
interviews. This was on the basis that there would be a lot of adjustment to accommodate the
interviewees. However, experience has shown that it can be helpful for the ODRA team to draft
Open Data Readiness Assessments: Lessons Learned
                                                                                                 16


the timetable and for the counterpart team to make requests for interviews on specific dates and
times – in practice in most countries most MDAs agree to the suggested time. This approach also
allows the timetable to be arranged so that officials responsible for cross-government policies
(such as copyright, charging and access to information laws) can be seen at the beginning and
then interviews with individual MDAs can test the extent to which these policies are being put
into practice.

Development Partners
3.11 Engaging with Development Partners can be a valuable source of information about the data
that the government holds, the issues in obtaining it and the attitudes of the relevant Ministries
– including the National Statistical Office. Often development partners – including World Bank
colleagues – have a long experience of trying to obtain data for their work, and in some cases
have supported projects to improve data collection in specific sectors. In some cases, data has
been marshalled with the help of development partners into a form in which it could be published
(such as the World Bank’s BOOST government finance tool) but a final pu sh is needed to secure
government agreement to actually publish it.

Information sharing activities
3.12 ODRAs have usually been associated with lots of education and information-sharing about
open data – both in associated presentations and workshops and during fact-finding interviews
themselves. Particular value has been found in:

          Starting the ODRA program with a presentation on the uses and value of open data
           aimed at government officials, including those to be interviewed later in the program.
           This saves time in interviews and gives participants the chance to think about open
           data issues before the interview. It also allows senior officials to signal their backing
           for the ODRA work at the beginning of the week and so help ensure fuller participation
           later. In some cases, the opening presentation has prompted other government data
           holders to come forward and ask to talk to the ODRA team.

          A public event – sometimes in the evening – to engage similarly with civil society and
           to give some publicity to the ODRA. The sponsors in many governments have been
           keen that this should be done, to get positive messages into the press about the
           government’s sponsorship of the initiative.

          Having ready a number of sector examples of the use and value of open data for the
           interviews with other MDAs – particularly examples where open data has help the
           corresponding MDA in another country achieve its objectives.
Open Data Readiness Assessments: Lessons Learned
                                                                                                17


          Sharing preliminary inventory of datasets identified during the ODRA broadly has
           proven very useful in some countries to attract attention from other development
           partners, NGOs and potential data users.

          Doing media interview, TV, press, radio in first days of the ODRA mission is also a very
           powerful way to introduce the topic into the country and get some momentum. In
           Sierra Leone, the team did more than 5 TV and radio interviews, thanks to one of the
           counterpart who was a form Journalist, this helped a lot during meetings with
           Government officials who were already informed.

Using evenings and weekends effectively
3.13 In some countries it was found that the culture and practical considerations (such as civil
society activists with full-time jobs) made it most useful to have some meetings in the evenings
or at weekends. For instance, in Antigua the counterparts suggested a public meeting in St. Johns
in the evening; this was very well attended, including by one Secretary of a major Ministry.
Similarly, in Trinidad and Tobago sessions for activists and for civil society organizations were
held on Saturdays when more potential participants were available.

Capture emerging conclusions during mission
3.14 In some ODRAs the team has taken some time – often as early as Wednesday evening in a
one-week ODRA mission – to brain-storm and capture the main messages and key
recommendations and actions for the ODRA report. While not intended to be exhaustive,
capturing these during the mission has enabled the main themes of the report to be identified
and the emerging conclusions presented to the client in the confident knowledge that they will
be reflected in the report. It also allows different members of the team to draft different section
in a consistent manner. Finally, it is a check on whether there are any big gaps in the team’s
knowledge that can be addressed before the end of the mission.

Scoring given more weight than intended
3.15 In the design of the ODRA methodology the Red/Yellow/Green designation and the
measures of relative importance had been intended mainly to indicate to decision-makers key
areas for attention. In version 3 of the ODRA “rubrics” were introduced to allow reviewers to
assign color ratings in a more consistent way. However, both some ODRA teams and, particularly,
some client governments have highlighted the “scores” themselves in ways that were not
intended. Not only may this have distracted from attention to the key messages of the report but
it led to some process difficulties – for instance when, on the basis of the first draft report, a
counterpart told his Minister that “there were no Reds”; this made it very difficult to change the
report when internal peer reviewers suggested that some of the Yellows should be downgraded
to Reds.
